# -*- coding: utf-8 -*-
"""PatchTST_ACB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u3PTzA7sz0ogptry2iPcL_7nnUSCqSiw

#Install and Import Libraries
"""

!pip install -q torch transformers accelerate einops optuna

import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

import random, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error

import torch
import torch.nn as nn
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import Dataset, DataLoader

import math, time
from transformers import PatchTSTConfig, PatchTSTForPrediction, PatchTSTForRegression
import torch.nn.functional as F

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""#Load dataset"""

dataset = pd.read_csv("dataset_ACB.csv")
dataset['date'] = pd.to_datetime(dataset['date'])
dataset.set_index('date', inplace=True)

dataset.drop(columns=['ticker', 'field'], inplace=True)

dataset.head()

numeric_vars = ['close','open', 'high', 'low','MA_10', 'MA_50', 'MA_200','GDP √ó 10^9 (USD)', 'CPI', 'usd_vnd']
for var in numeric_vars:
  dataset[var] = pd.to_numeric(dataset[var], errors='coerce')

dataset.shape, dataset.info()

dataset.head()

"""#Train / Valid/ Test Split and Preprocess"""

data = dataset.copy()

# L∆∞u data g·ªëc TR∆Ø·ªöC KHI SCALE
data_original = data.copy()

train_ratio = 0.6
valid_ratio = 0.15

n = len(data)
train_end = int(train_ratio * n)
valid_end = int((train_ratio + valid_ratio) * n)

train_df = data.iloc[:train_end].copy()
valid_df = data.iloc[train_end:valid_end].copy()
test_df  = data.iloc[valid_end:].copy()

# Data g·ªëc cho evaluation
train_df_original = data_original.iloc[:train_end].copy()
valid_df_original = data_original.iloc[train_end:valid_end].copy()
test_df_original  = data_original.iloc[valid_end:].copy()

print(f"Total: {n}")
print(f"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}")

# Features d√πng RobustScaler (KH√îNG bao g·ªìm close)
features_robust = ['open', 'high', 'low','MA_10', 'MA_50', 'MA_200','GDP √ó 10^9 (USD)', 'CPI', 'usd_vnd']

# RobustScaler cho c√°c features kh√°c
scaler_robust = RobustScaler().fit(train_df[features_robust])

train_df[features_robust] = scaler_robust.transform(train_df[features_robust])
valid_df[features_robust] = scaler_robust.transform(valid_df[features_robust])
test_df[features_robust]  = scaler_robust.transform(test_df[features_robust])

#StandardScaler cho close
scaler_close = StandardScaler().fit(train_df[['close']])

train_df['close'] = scaler_close.transform(train_df[['close']])
valid_df['close'] = scaler_close.transform(valid_df[['close']])
test_df['close']  = scaler_close.transform(test_df[['close']])

#KH√îNG scale pct_change - gi·ªØ nguy√™n
print(f"\nScaling summary:")
print(f"  RobustScaler: {features_robust}")
print(f"  StandardScaler: ['close']")

"""#Dataset, DataLoader"""

class StockDataset(Dataset):
    def __init__(self, df, df_original, feature_cols, horizons=[1, 4, 8, 12, 24], window=36):
        """
        Args:
            df: DataFrame ƒë√£ ƒë∆∞·ª£c scale
            df_original: DataFrame G·ªêC (ch∆∞a scale) ƒë·ªÉ l·∫•y gi√° th·ª±c
            feature_cols: C√°c c·ªôt feature (bao g·ªìm pct_change)
            horizons: C√°c horizon c·∫ßn d·ª± ƒëo√°n
            window: S·ªë ng√†y lookback (L)
        """
        self.samples = []
        self.horizons = horizons

        features = df[feature_cols].values
        close_original = df_original['close'].values  # Gi√° G·ªêC ch∆∞a scale

        n = len(df)
        for i in range(n - window - max(horizons)):
            X = features[i : i + window]

            # Base price G·ªêC (ng√†y cu·ªëi window)
            base_price = close_original[i + window - 1]

            # Target: cumulative pct_change t·ª´ ng√†y L ƒë·∫øn ng√†y L+H
            y = []
            actual_prices = []
            for h in horizons:
                future_price = close_original[i + window + h - 1]
                cumulative_return = (future_price - base_price) / base_price if base_price != 0 else 0
                y.append(cumulative_return)
                actual_prices.append(future_price)

            self.samples.append((X, y, base_price, actual_prices))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        X, y, base_price, actual_prices = self.samples[idx]
        return (torch.tensor(X, dtype=torch.float32),
                torch.tensor(y, dtype=torch.float32),
                torch.tensor(base_price, dtype=torch.float32),
                torch.tensor(actual_prices, dtype=torch.float32))

"""#Early Stopping"""

# Early Stopping
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.best = None
        self.count = 0
        self.best_state = None

    def step(self, metric, model):
        improved = (self.best is None) or (metric < self.best - self.min_delta)
        if improved:
            self.best = metric
            self.count = 0
            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            self.count += 1
        return improved, self.count >= self.patience

"""#Optimize HyperParameters"""

# C·∫•u h√¨nh
H_LIST = [1, 4, 7, 10, 14, 21]  # Horizons c·∫ßn d·ª± ƒëo√°n

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

def objective(trial):
    # === HYPERPARAMETERS C·∫¶N T·ªêI ∆ØU ===
    L = trial.suggest_categorical('L', [12, 24, 36, 48, 60, 72, 84, 96])
    patch_len = trial.suggest_categorical('patch_len', [4, 8, 12, 16])
    stride = trial.suggest_categorical('stride', [2, 4, 6, 8])
    d_model = trial.suggest_categorical('d_model', [32, 64, 128, 256])
    n_heads = trial.suggest_categorical('n_heads', [2, 4, 8])
    num_encoder_layers = trial.suggest_int('num_encoder_layers', 1, 4)
    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)

    # === R√ÄNG BU·ªòC ===
    # 1. d_model ph·∫£i chia h·∫øt cho n_heads
    if d_model % n_heads != 0:
        raise optuna.TrialPruned()

    # 2. patch_len ph·∫£i <= L
    if patch_len > L:
        raise optuna.TrialPruned()

    # 3. stride ph·∫£i <= patch_len
    if stride > patch_len:
        raise optuna.TrialPruned()

    # 4. S·ªë patches ph·∫£i >= 1
    num_patches = (L - patch_len) // stride + 1
    if num_patches < 1:
        raise optuna.TrialPruned()

    # === T·∫†O DATASET ===
    train_dataset = StockDataset(train_df, train_df_original, feature_cols, horizons=H_LIST, window=L)
    valid_dataset = StockDataset(valid_df, valid_df_original, feature_cols, horizons=H_LIST, window=L)

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        raise optuna.TrialPruned()

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    # === T·∫†O MODEL ===
    num_features = len(feature_cols)

    config = PatchTSTConfig(
        num_input_channels=num_features,
        context_length=L,
        patch_len=patch_len,
        stride=stride,
        d_model=d_model,
        n_heads=n_heads,
        num_encoder_layers=num_encoder_layers,
        dropout=dropout,
        num_targets=len(H_LIST),
    )

    model = PatchTSTForRegression(config).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    # === TRAINING LOOP ===
    for epoch in range(1, 51):
        model.train()
        for X_batch, y_batch, _, _ in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            out = model(past_values=X_batch)
            loss = criterion(out.regression_outputs, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch, _, _ in valid_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                out = model(past_values=X_batch)
                val_loss += criterion(out.regression_outputs, y_batch).item()
        val_loss /= len(valid_loader)

        # Pruning
        trial.report(val_loss, epoch)
        if trial.should_prune():
            raise optuna.TrialPruned()

        improved, should_stop = early_stopping.step(val_loss, model)
        if should_stop:
            break

    return early_stopping.best

study = optuna.create_study(
    direction='minimize',
    sampler=TPESampler(seed=42),
    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)
)

study.optimize(objective, n_trials=50, show_progress_bar=True)

# K·∫øt qu·∫£
print(f"\n{'='*50}")
print(f"BEST PARAMS:")
print(f"  L: {study.best_params['L']}")
print(f"  patch_len: {study.best_params['patch_len']}")
print(f"  stride: {study.best_params['stride']}")
print(f"  d_model: {study.best_params['d_model']}")
print(f"  n_heads: {study.best_params['n_heads']}")
print(f"  num_encoder_layers: {study.best_params['num_encoder_layers']}")
print(f"  dropout: {study.best_params['dropout']}")
print(f"  Valid Loss: {study.best_value:.6f}")

"""#Training With PatchTST"""

# === S·ª¨ D·ª§NG BEST PARAMS T·ª™ OPTUNA (tr·ª´ L) ===
best_patch_len = study.best_params['patch_len']
best_stride = study.best_params['stride']
best_d_model = study.best_params['d_model']
best_n_heads = study.best_params['n_heads']
best_num_encoder_layers = study.best_params['num_encoder_layers']
best_dropout = study.best_params['dropout']

print(f"Using Optuna best params:")
print(f"  patch_len={best_patch_len}, stride={best_stride}")
print(f"  d_model={best_d_model}, n_heads={best_n_heads}")
print(f"  num_encoder_layers={best_num_encoder_layers}, dropout={best_dropout}")

# C·∫•u h√¨nh
L_VALUES = [12, 24, 36, 48, 60, 72, 84, 96]  # Context length (window size) - PatchTST th∆∞·ªùng c·∫ßn L l·ªõn h∆°n
H_LIST = [1, 4, 7, 10, 14, 21]  # Horizons c·∫ßn d·ª± ƒëo√°n

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

for L in L_VALUES:
    print(f"\n{'='*60}")
    print(f"TRAINING PatchTST WITH L = {L}")
    print(f"{'='*60}")

    # Ki·ªÉm tra r√†ng bu·ªôc: patch_len ph·∫£i <= L
    if best_patch_len > L:
        print(f"Skipping L={L}: patch_len ({best_patch_len}) > L ({L})")
        continue

    # T·∫°o dataset
    train_dataset = StockDataset(train_df, train_df_original, feature_cols, horizons=H_LIST, window=L)
    valid_dataset = StockDataset(valid_df, valid_df_original, feature_cols, horizons=H_LIST, window=L)

    print(f"Train samples: {len(train_dataset)}, Valid samples: {len(valid_dataset)}")

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        print(f"Skipping L={L}: Not enough samples")
        continue

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    # C·∫§U H√åNH PatchTSTForRegression v·ªõi params t·ª´ Optuna
    num_features = len(feature_cols)

    config = PatchTSTConfig(
        num_input_channels=num_features,
        context_length=L,
        patch_len=best_patch_len,
        stride=best_stride,
        d_model=best_d_model,
        n_heads=best_n_heads,
        num_encoder_layers=best_num_encoder_layers,
        dropout=best_dropout,
        num_targets=len(H_LIST),
    )

    model = PatchTSTForRegression(config).to(device)
    print(f"PatchTST parameters: {model.num_parameters():,}")

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    EPOCHS = 50
    train_losses, valid_losses = [], []

    for epoch in range(1, EPOCHS + 1):
        # TRAINING
        model.train()
        running_train = 0.0
        for X_batch, y_batch, _, _ in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()
            out = model(past_values=X_batch)
            preds = out.regression_outputs

            loss = criterion(preds, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_train += loss.item() * X_batch.size(0)

        avg_train = running_train / len(train_dataset)
        train_losses.append(avg_train)

        # VALIDATION
        model.eval()
        running_valid = 0.0
        with torch.no_grad():
            for X_batch, y_batch, _, _ in valid_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device)

                out = model(past_values=X_batch)
                preds = out.regression_outputs

                loss = criterion(preds, y_batch)
                running_valid += loss.item() * X_batch.size(0)

        avg_valid = running_valid / len(valid_dataset)
        valid_losses.append(avg_valid)

        scheduler.step(avg_valid)
        improved, stop = early_stopping.step(avg_valid, model)

        if epoch % 10 == 0 or improved:
            mark = "*" if improved else ""
            print(f"Epoch {epoch:3d} | Train: {avg_train:.6f} | Valid: {avg_valid:.6f} {mark}")

        if stop:
            print(f"Early stopping at epoch {epoch}")
            break

    final_valid_loss = early_stopping.best
    results_by_L[L] = {
        'valid_loss': final_valid_loss,
        'train_losses': train_losses,
        'valid_losses': valid_losses,
        'model_state': early_stopping.best_state,
        'patch_len': best_patch_len,
        'stride': best_stride
    }

    if final_valid_loss < best_valid_loss:
        best_valid_loss = final_valid_loss
        best_L = L
        best_model_state = early_stopping.best_state

    print(f"\nL={L} completed. Best valid loss: {final_valid_loss:.6f}")

print(f"\n{'='*60}")
print(f"BEST L = {best_L} with valid loss = {best_valid_loss:.6f}")
print(f"{'='*60}")

"""#Plot Traning, Valid Loss"""

if best_L is not None:
    plt.figure(figsize=(10, 4))
    plt.plot(results_by_L[best_L]['train_losses'], label='Train Loss')
    plt.plot(results_by_L[best_L]['valid_losses'], label='Valid Loss')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.title(f'PatchTST Training - L={best_L}')
    plt.legend()
    plt.grid(True)
    plt.show()

plt.figure(figsize=(8, 5))

# So s√°nh valid loss c√°c L
L_list = list(results_by_L.keys())
valid_losses_list = [results_by_L[L]['valid_loss'] for L in L_list]

bars = plt.bar(range(len(L_list)), valid_losses_list, color='steelblue')
plt.xticks(range(len(L_list)), [f'L={L}' for L in L_list])
plt.ylabel('Best Valid Loss')
plt.title('Validation Loss by Window Size (L)')
plt.axhline(y=best_valid_loss, color='red', linestyle='--', label=f'Best: L={best_L}')

# Highlight best L
best_idx = L_list.index(best_L)
bars[best_idx].set_color('green')
plt.legend()

plt.tight_layout()
plt.show()

# In b·∫£ng so s√°nh
print(f"\n{'='*40}")
print(f"{'L':>6} | {'Valid Loss':>12} | {'Rank':>6}")
print(f"{'='*40}")
sorted_results = sorted(results_by_L.items(), key=lambda x: x[1]['valid_loss'])
for rank, (L, res) in enumerate(sorted_results, 1):
    marker = " ‚Üê Best" if L == best_L else ""
    print(f"{L:>6} | {res['valid_loss']:>12.6f} | {rank:>6}{marker}")

"""#Evaluate on Test with Best L"""

print(f"\n{'='*60}")
print(f"EVALUATING ON TEST SET WITH BEST L = {best_L}")
print(f"{'='*60}")

# T·∫°o test dataset
test_dataset = StockDataset(
    test_df, test_df_original, feature_cols,
    horizons=H_LIST, window=best_L
)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"Test samples: {len(test_dataset)}")

# L·∫•y l·∫°i patch_len v√† stride t·ª´ results
patch_len = results_by_L[best_L]['patch_len']
stride = results_by_L[best_L]['stride']

# T·∫°o l·∫°i model - D√ôNG PatchTSTForRegression
num_features = len(feature_cols)

config = PatchTSTConfig(
        num_input_channels=num_features,
        context_length=best_L,
        patch_len=best_patch_len,
        stride=best_stride,
        d_model=best_d_model,
        n_heads=best_n_heads,
        num_encoder_layers=best_num_encoder_layers,
        dropout=best_dropout,
        num_targets=len(H_LIST),
        )

model = PatchTSTForRegression(config).to(device)
model.load_state_dict(best_model_state)
model.eval()

# Thu th·∫≠p predictions
all_preds = []
all_targets = []
all_base_prices = []
all_actual_prices = []

with torch.no_grad():
    for X_batch, y_batch, base_batch, actual_batch in test_loader:
        X_batch = X_batch.to(device)

        out = model(past_values=X_batch)
        preds = out.regression_outputs  # [batch, 5] - KH√îNG C·∫¶N slice

        all_preds.append(preds.cpu().numpy())
        all_targets.append(y_batch.numpy())
        all_base_prices.append(base_batch.numpy())
        all_actual_prices.append(actual_batch.numpy())

all_preds = np.concatenate(all_preds, axis=0)
all_targets = np.concatenate(all_targets, axis=0)
all_base_prices = np.concatenate(all_base_prices, axis=0)
all_actual_prices = np.concatenate(all_actual_prices, axis=0)

print(f"Predictions shape: {all_preds.shape}")
print(f"Targets shape: {all_targets.shape}")

print("\n=== TEST SET RESULTS (ACTUAL PRICE - VND) ===")

# Chuy·ªÉn cumulative return v·ªÅ gi√°: pred_price = base_price * (1 + cumulative_return)
pred_prices = np.zeros_like(all_preds)
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_preds[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

for i, h in enumerate(H_LIST):
    actual = all_actual_prices[:, i]
    pred   = pred_prices[:, i]

    # Mean price (d√πng ƒë·ªÉ scale v·ªÅ 0‚Äì1)
    mean_price = actual.mean()

    # Metrics g·ªëc (VND)
    mae  = mean_absolute_error(actual, pred)
    rmse = np.sqrt(mean_squared_error(actual, pred))
    mape = np.mean(np.abs((actual - pred) / actual)) * 100
    r2   = r2_score(actual, pred)

    # Metrics normalized (0 ‚Üí 1)
    mae_norm  = mae / mean_price
    rmse_norm = rmse / mean_price

    print(
        f"H={h:3d}: "
        f"MAE={mae:,.2f} VND | "
        f"RMSE={rmse:,.2f} VND | "
        f"MAE_norm={mae_norm:.5f} | "
        f"RMSE_norm={rmse_norm:.5f} | "
        f"MAPE={mape:.2f}% | "
        f"R2={r2:.5f}"
    )

"""#Visualize Predictions"""

print("\n" + "="*70)
print("SAMPLE PREDICTIONS (Last 5 samples)")
print("="*70)

for sample_idx in range(-5, 0):
    base = all_base_prices[sample_idx]
    print(f"\nüìä Sample {sample_idx}: Base Price = {base:,.0f} VND")
    print("-" * 60)

    for i, h in enumerate(H_LIST):
        actual_price = all_actual_prices[sample_idx, i]
        pred_pct = all_preds[sample_idx, i]  # ƒê·ªïi all_y_pred ‚Üí all_preds

        # T√≠nh gi√° d·ª± ƒëo√°n: base + (base * cumulative_return)
        delta = base * pred_pct
        pred_price = base + delta

        error = abs(actual_price - pred_price)
        error_pct = error / actual_price * 100

        direction = "üìà" if pred_pct > 0 else "üìâ"

        print(f"  H={h:3d}: cumulative_return={pred_pct:+.4f} ({pred_pct*100:+.2f}%) {direction}")
        print(f"         Delta={delta:+,.0f} VND")
        print(f"         Actual={actual_price:>10,.0f} VND | Pred={pred_price:>10,.0f} VND | Error={error:>8,.0f} VND ({error_pct:.2f}%)")

# T√≠nh pred_prices tr∆∞·ªõc khi plot
pred_prices = np.zeros_like(all_preds)  # [N, 5]
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_preds[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

# Ki·ªÉm tra shape
print(f"pred_prices shape: {pred_prices.shape}")
print(f"all_actual_prices shape: {all_actual_prices.shape}")

# Plot
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, h in enumerate(H_LIST):
    if i >= len(axes):
        break

    actual = all_actual_prices[:, i]
    pred = pred_prices[:, i]

    axes[i].scatter(actual, pred, alpha=0.5, s=20)

    # ƒê∆∞·ªùng perfect prediction
    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')

    axes[i].set_xlabel('Actual Price (VND)')
    axes[i].set_ylabel('Predicted Price (VND)')
    axes[i].set_title(f'Horizon H={h}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

# ·∫®n subplot th·ª´a
if len(H_LIST) < len(axes):
    axes[-1].axis('off')

plt.suptitle('Actual vs Predicted Prices', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Plot predictions vs actual cho m·ªôt v√†i horizons
fig, axes = plt.subplots(len(H_LIST), 1, figsize=(14, 3*len(H_LIST)))

for i, h in enumerate(H_LIST):
    ax = axes[i] if len(H_LIST) > 1 else axes

    pred_prices = all_base_prices * (1 + all_preds[:, i])
    actual_prices = all_actual_prices[:, i]

    ax.plot(actual_prices, label='Actual', alpha=0.7)
    ax.plot(pred_prices, label='Predicted', alpha=0.7)
    ax.set_title(f'Horizon H={h}')
    ax.set_xlabel('Sample')
    ax.set_ylabel('Price')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

torch.save({
    'model_state_dict': best_model_state,
    'best_L': best_L,
    'scaler_robust': scaler_robust,
    'scaler_close': scaler_close,
    'features_robust': features_robust,
    'feature_cols': feature_cols,
    'horizons': H_LIST,
    'model_type': 'PatchTSTForRegression',
    'config': {
        'num_input_channels': num_features,
        'context_length': best_L,
        'patch_len': patch_len,
        'stride': stride,
        'd_model': 128,
        'n_heads': 4,
        'num_encoder_layers': 2,
        'dropout': 0.3,
        'num_targets': len(H_LIST),
    }
}, f'patchtst_L{best_L}.pth')

print(f"\nModel saved as 'patchtst_L{best_L}.pth'")