# -*- coding: utf-8 -*-
"""Dataset_collected_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJrN2mh4d87qdUU38oJ_rab2umPz8-QF

#Download and import Libraries
"""

!pip install requests tqdm

import pandas as pd
import re
import numpy as np

import requests
import json
import csv
import time
import re
from tqdm import tqdm

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.tokenize import word_tokenize

nltk.download('all')

!pip install underthesea

"""#Load Dataset"""

df = pd.read_excel("texts.xlsx")
df.to_csv("texts.csv", index=False, encoding="utf-8-sig")

df = pd.read_csv("texts.csv")
df.head(50)

df.tail(50)

print("C√°c c·ªôt hi·ªán t·∫°i trong 'df':")
display(df.columns)

#ƒë·ªïi t√™n
df_renamed = df.rename(columns={'Th·ªùi gian': 'date', 'Ngu·ªìn': 'url', 'Ti√™u ƒë·ªÅ': 'title','N·ªôi dung' : 'content'})

print("C√°c c·ªôt sau khi ƒë·ªïi t√™n trong 'df_renamed':")
display(df_renamed.columns)

df_renamed['date'] = pd.to_datetime(df_renamed['date'])
df_renamed = df_renamed.set_index('date')

display(df_renamed.head())

df_renamed.info()

df_filtered = df_renamed[df_renamed.index <= '2025-10-20'].copy()

print("Ki·ªÉm tra th√¥ng tin DataFrame sau khi l·ªçc:")
display(df_filtered.info())

df_filtered.head(50)

df_filtered.tail(50)

# Rows thi·∫øu title
print("Rows thi·∫øu title:")
display(df_filtered[df_filtered['title'].isna()])

# Rows thi·∫øu content
print("\nRows thi·∫øu content:")
display(df_filtered[df_filtered['content'].isna()])

# ƒê·∫øm s·ªë tin t·ª©c m·ªói ng√†y
news_per_day = df_filtered.groupby(df_filtered.index.date).size()

print("TH·ªêNG K√ä S·ªê TIN T·ª®C M·ªñI NG√ÄY:")
print(news_per_day.describe())

print(f"\nMin: {news_per_day.min()} tin/ng√†y")
print(f"Max: {news_per_day.max()} tin/ng√†y")
print(f"Mean: {news_per_day.mean():.1f} tin/ng√†y")

# Ng√†y c√≥ nhi·ªÅu tin nh·∫•t
print(f"\nNg√†y c√≥ nhi·ªÅu tin nh·∫•t:")
print(news_per_day.nlargest(10))

# Ng√†y c√≥ √≠t tin nh·∫•t
print(f"\nNg√†y ch·ªâ c√≥ 1 tin:")
print(f"{(news_per_day == 1).sum()} ng√†y")

# Histogram
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.hist(news_per_day, bins=20, edgecolor='black')
plt.xlabel('S·ªë tin t·ª©c / ng√†y')
plt.ylabel('S·ªë ng√†y')
plt.title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng tin t·ª©c m·ªói ng√†y')
plt.show()

# T·∫°o DataFrame th·ªëng k√™
summary = pd.DataFrame({
    'news_count': news_per_day
})

print(f"T·ªïng s·ªë ng√†y c√≥ tin: {len(news_per_day)}")
print(f"T·ªïng s·ªë tin t·ª©c: {news_per_day.sum()}")
print(f"\nPh√¢n b·ªë:")
print(f"  - Ng√†y c√≥ 1 tin:   {(news_per_day == 1).sum()}")
print(f"  - Ng√†y c√≥ 2-5 tin: {((news_per_day >= 2) & (news_per_day <= 5)).sum()}")
print(f"  - Ng√†y c√≥ >5 tin:  {(news_per_day > 5).sum()}")

"""#ƒê·∫æM TOKENS C·ª¶A C·ªòT CONTENT"""

from transformers import AutoTokenizer
import pandas as pd

# Load tokenizers
print("Loading tokenizers...")
tok_phobert = AutoTokenizer.from_pretrained("vinai/phobert-base")
tok_vn_embed = AutoTokenizer.from_pretrained("AITeamVN/Vietnamese_Embedding")
tok_vn_doc = AutoTokenizer.from_pretrained("dangvantuan/vietnamese-document-embedding", trust_remote_code=True)

# ƒê·∫øm tokens
df_filtered['tok_phobert'] = df_filtered['content'].apply(lambda x: len(tok_phobert.encode(str(x))) if pd.notna(x) else 0)
df_filtered['tok_vn_embed'] = df_filtered['content'].apply(lambda x: len(tok_vn_embed.encode(str(x))) if pd.notna(x) else 0)
df_filtered['tok_vn_doc'] = df_filtered['content'].apply(lambda x: len(tok_vn_doc.encode(str(x))) if pd.notna(x) else 0)

# Th·ªëng k√™
print("\n" + "="*60)
print("MODEL                          | MAX  | % FIT  | MAX IN DATA")
print("="*60)
print(f"PhoBERT-base/large             | 256  | {(df_filtered['tok_phobert']<=256).mean()*100:5.1f}% | {df_filtered['tok_phobert'].max():,}")
print(f"Vietnamese_Embedding (AITeamVN)| 2048 | {(df_filtered['tok_vn_embed']<=2048).mean()*100:5.1f}% | {df_filtered['tok_vn_embed'].max():,}")
print(f"vietnamese-document-embedding  | 8192 | {(df_filtered['tok_vn_doc']<=8192).mean()*100:5.1f}% | {df_filtered['tok_vn_doc'].max():,}")
print("="*60)

"""# üìä B√°o C√°o Ph√¢n T√≠ch Token Count

## K·∫øt Qu·∫£ Th·ªëng K√™
```
============================================================
MODEL                          | MAX  | % FIT  | MAX IN DATA
============================================================
PhoBERT-base/large             | 256  |  99.6% | 530
Vietnamese_Embedding (AITeamVN)| 2048 | 100.0% | 526
vietnamese-document-embedding  | 8192 | 100.0% | 526
============================================================
```

---

## 1. Gi·∫£i Th√≠ch C√°c C·ªôt

| C·ªôt | √ù nghƒ©a |
|-----|---------|
| **MODEL** | T√™n model embedding ƒë∆∞·ª£c ƒë√°nh gi√° |
| **MAX** | S·ªë tokens t·ªëi ƒëa m√† model c√≥ th·ªÉ x·ª≠ l√Ω trong 1 l·∫ßn |
| **% FIT** | Ph·∫ßn trƒÉm vƒÉn b·∫£n trong dataset n·∫±m trong gi·ªõi h·∫°n MAX |
| **MAX IN DATA** | S·ªë tokens c·ªßa vƒÉn b·∫£n d√†i nh·∫•t trong dataset |

---

## 2. Ph√¢n T√≠ch T·ª´ng Model

### 2.1. PhoBERT-base/large

| Th√¥ng s·ªë | Gi√° tr·ªã |
|----------|---------|
| Gi·ªõi h·∫°n MAX | 256 tokens |
| % FIT | 99.6% |
| MAX IN DATA | 530 tokens |

**Gi·∫£i th√≠ch:**
- ‚úÖ **99.6%** vƒÉn b·∫£n c√≥ ƒë·ªô d√†i ‚â§ 256 tokens ‚Üí Gi·ªØ nguy√™n, kh√¥ng m·∫•t th√¥ng tin
- ‚ùå **0.4%** vƒÉn b·∫£n c√≥ ƒë·ªô d√†i > 256 tokens ‚Üí B·ªã **truncate (c·∫Øt b·ªè)** ph·∫ßn v∆∞·ª£t qu√°

**H·∫≠u qu·∫£ v·ªõi 0.4% vƒÉn b·∫£n b·ªã c·∫Øt:**
```
VƒÉn b·∫£n 530 tokens:  [1] [2] [3] ... [256] | [257] [258] ... [530]
                                      ‚Üë
                                 C·∫Øt t·∫°i ƒë√¢y
                         
Gi·ªØ l·∫°i: [1]...[256]   ‚Üí 48% n·ªôi dung
M·∫•t ƒëi:  [257]...[530] ‚Üí 52% n·ªôi dung
```

---

### 2.2. Vietnamese_Embedding (AITeamVN)

| Th√¥ng s·ªë | Gi√° tr·ªã |
|----------|---------|
| Gi·ªõi h·∫°n MAX | 2048 tokens |
| % FIT | 100% |
| MAX IN DATA | 526 tokens |

**Gi·∫£i th√≠ch:**
- ‚úÖ **100%** vƒÉn b·∫£n n·∫±m trong gi·ªõi h·∫°n
- ‚úÖ VƒÉn b·∫£n d√†i nh·∫•t (526) < Gi·ªõi h·∫°n (2048)
- ‚úÖ **Kh√¥ng m·∫•t th√¥ng tin**

---

### 2.3. vietnamese-document-embedding

| Th√¥ng s·ªë | Gi√° tr·ªã |
|----------|---------|
| Gi·ªõi h·∫°n MAX | 8192 tokens |
| % FIT | 100% |
| MAX IN DATA | 526 tokens |

**Gi·∫£i th√≠ch:**
- ‚úÖ **100%** vƒÉn b·∫£n n·∫±m trong gi·ªõi h·∫°n
- ‚úÖ **Kh√¥ng m·∫•t th√¥ng tin**

---

## 3. T·∫°i Sao MAX IN DATA Kh√°c Nhau (530 vs 526)?

M·ªói tokenizer t√°ch t·ª´ kh√°c nhau:

| C√¢u v√≠ d·ª• | PhoBERT | Vietnamese_Embedding |
|-----------|---------|---------------------|
| "TƒÉng 2.5%" | ["TƒÉng", "2", ".", "5", "%"] = 5 tokens | ["TƒÉng", "2.5%"] = 2 tokens |

‚Üí PhoBERT t√°ch nh·ªè h∆°n ‚Üí c√πng vƒÉn b·∫£n nh∆∞ng nhi·ªÅu tokens h∆°n

---

## 4. So S√°nh T·ªïng Quan

| Ti√™u ch√≠ | PhoBERT | Vietnamese_Embedding | vn-document-embedding |
|----------|:-------:|:--------------------:|:---------------------:|
| Max tokens | 256 | 2048 | 8192 |
| Output dim | 768/1024 | 1024 | 768 |
| % Data fit | 99.6% | **100%** | **100%** |
| M·∫•t th√¥ng tin? | C√≥ (0.4%) | **Kh√¥ng** | **Kh√¥ng** |

#Preprocessing

- Lo·∫°i b·ªè k√Ω t·ª± l·ªói: \n, \t
- Lo·∫°i b·ªè emoji (n·∫øu c√≥)
- Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
- Thay k√≠ t·ª± HTML (&#39;, &quot;, &amp;, ‚Ä¶)
- Gi·ªØ nguy√™n k√Ω t·ª± quan tr·ªçng c·ªßa kinh t·∫ø: % . , / -
- Kh√¥ng x√≥a d·∫•u ti·∫øng Vi·ªát
- Kh√¥ng x√≥a s·ªë
"""

!pip install wordcloud

import re
import html
import pandas as pd

def minimal_clean(text):
    """Preprocessing t·ªëi thi·ªÉu cho model embedding hi·ªán ƒë·∫°i"""
    if pd.isna(text):
        return ""

    text = str(text)

    # 1. Gi·∫£i m√£ HTML entities
    text = html.unescape(text)

    # 2. B·ªè HTML tags
    text = re.sub(r'<[^>]+>', ' ', text)

    # 3. B·ªè URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # 4. B·ªè emoji v√† k√Ω t·ª± l·∫° (gi·ªØ l·∫°i ch·ªØ, s·ªë, d·∫•u c√¢u c∆° b·∫£n, ti·∫øng Vi·ªát)
    text = re.sub(
        r"[^0-9a-zA-Z√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ"
        r"√ç√å·ªàƒ®·ªä√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê"
        r"√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá"
        r"√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë"
        r"%\.\,\/\-\+\$‚Ç´\(\)\:\!\?\s]",
        " ",
        text
    )

    # 5. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = ' '.join(text.split())

    return text

# Test
text = "C·ªï phi·∫øu VNM tƒÉng +5.2%!\nHPG gi·∫£m -2% (v·ªÅ m·ª©c $85). P/E: 15.3"

print("G·ªêC:", repr(text))
print("SAU:", minimal_clean(text))

df_filtered.head()

# √Åp d·ª•ng
df_filtered['content_clean'] = df_filtered['content'].apply(minimal_clean)

df_filtered

# C√°ch 1: ƒê·∫øm s·ªë NaN
print(f"S·ªë NaN: {df_filtered['content_clean'].isna().sum()}")

# C√°ch 2: Xem chi ti·∫øt
print(f"T·ªïng rows: {len(df_filtered)}")
print(f"Non-null:  {df_filtered['content_clean'].notna().sum()}")
print(f"NaN:       {df_filtered['content_clean'].isna().sum()}")

# C√°ch 3: Xem rows b·ªã NaN
df_filtered[df_filtered['content_clean'].isna()]

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Gi·∫£ s·ª≠ c·ªôt ƒë√£ l√† content_clean
text = " ".join(df_filtered["content_clean"].astype(str).tolist())
len(text)

wc = WordCloud(
    width=1200,
    height=600,
    background_color="white"   # m√†u n·ªÅn, c√≤n m√†u ch·ªØ ƒë·ªÉ m·∫∑c ƒë·ªãnh
).generate(text)

plt.figure(figsize=(14, 7))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

mask_2024 = df_filtered.index >= "2024-01-01"
text_2024 = " ".join(df_filtered.loc[mask_2024, "content_clean"].astype(str))

wc_2024 = WordCloud(
    width=1200,
    height=600,
    background_color="white"
).generate(text_2024)

plt.figure(figsize=(14, 7))
plt.imshow(wc_2024, interpolation="bilinear")
plt.axis("off")
plt.show()

"""#Download preprecessing dataset"""

df_filtered.to_csv("collected_news_preprocess.csv", index=True, encoding="utf-8-sig")

import pandas as pd

# 1. T·∫°o d√£y t·∫•t c·∫£ c√°c ng√†y trong kho·∫£ng th·ªùi gian
start_date = '2020-01-01'
end_date = '2025-10-20'
all_dates = pd.date_range(start=start_date, end=end_date, freq='D')

print(f"T·ªïng s·ªë ng√†y t·ª´ {start_date} ƒë·∫øn {end_date}: {len(all_dates)}")

# 2. L·∫•y c√°c ng√†y c√≥ trong dataset (t·ª´ index)
news_dates = pd.to_datetime(df_filtered.index).unique()

print(f"S·ªë ng√†y c√≥ tin t·ª©c trong dataset: {len(news_dates)}")

# 3. T√¨m c√°c ng√†y b·ªã thi·∫øu
missing_dates = set(all_dates) - set(news_dates)
missing_dates = sorted(missing_dates)

print(f"\nüìå S·ªë ng√†y b·ªã thi·∫øu: {len(missing_dates)}")

# 4. Hi·ªÉn th·ªã c√°c ng√†y b·ªã thi·∫øu
if missing_dates:
    print("\nüîç Danh s√°ch ng√†y b·ªã thi·∫øu:")
    for date in missing_dates[:20]:  # Hi·ªÉn th·ªã 20 ng√†y ƒë·∫ßu
        print(f"  - {date.strftime('%Y-%m-%d')} ({date.strftime('%A')})")

    if len(missing_dates) > 20:
        print(f"  ... v√† {len(missing_dates) - 20} ng√†y kh√°c")
else:
    print("\n‚úÖ Kh√¥ng c√≥ ng√†y n√†o b·ªã thi·∫øu!")