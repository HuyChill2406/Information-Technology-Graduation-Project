# -*- coding: utf-8 -*-
"""LSTM_ACB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112Awg_JIz5NnFKTuUt5twNHGdAJze-Ot

#Install and Import Libraries
"""

!pip install -q torch optuna

import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

import random, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error

import torch
import torch.nn as nn
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import Dataset, DataLoader

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""#Load dataset"""

dataset = pd.read_csv("dataset_ACB.csv")
dataset['date'] = pd.to_datetime(dataset['date'])
dataset.set_index('date', inplace=True)

dataset = dataset.drop(columns=['ticker','field'])

dataset.head()

numeric_vars = ['close','open','high','low','MA_10','MA_50','MA_200','GDP √ó 10^9 (USD)','CPI','usd_vnd']
for var in numeric_vars:
  dataset[var] = pd.to_numeric(dataset[var], errors='coerce')

dataset.shape, dataset.info()

dataset.head()

dataset.tail()

"""#Train / Valid/ Test Split and Preprocess"""

data = dataset.copy()

# L∆∞u data g·ªëc TR∆Ø·ªöC KHI SCALE
data_original = data.copy()

train_ratio = 0.6
valid_ratio = 0.15

n = len(data)
train_end = int(train_ratio * n)
valid_end = int((train_ratio + valid_ratio) * n)

train_df = data.iloc[:train_end].copy()
valid_df = data.iloc[train_end:valid_end].copy()
test_df  = data.iloc[valid_end:].copy()

# Data g·ªëc cho evaluation
train_df_original = data_original.iloc[:train_end].copy()
valid_df_original = data_original.iloc[train_end:valid_end].copy()
test_df_original  = data_original.iloc[valid_end:].copy()

print(f"Total: {n}")
print(f"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}")

# Features d√πng RobustScaler (KH√îNG bao g·ªìm close)
features_robust = ['open','high','low','MA_10','MA_50','MA_200','GDP √ó 10^9 (USD)','CPI','usd_vnd']

# RobustScaler cho c√°c features kh√°c
scaler_robust = RobustScaler().fit(train_df[features_robust])

train_df[features_robust] = scaler_robust.transform(train_df[features_robust])
valid_df[features_robust] = scaler_robust.transform(valid_df[features_robust])
test_df[features_robust]  = scaler_robust.transform(test_df[features_robust])

#StandardScaler cho close
scaler_close = StandardScaler().fit(train_df[['close']])

train_df['close'] = scaler_close.transform(train_df[['close']])
valid_df['close'] = scaler_close.transform(valid_df[['close']])
test_df['close']  = scaler_close.transform(test_df[['close']])

print(f"\nScaling summary:")
print(f"  RobustScaler: {features_robust}")
print(f"  StandardScaler: ['close']")

"""# Dataset class"""

class StockDataset(Dataset):
    def __init__(self, df, df_original, feature_cols, horizons=[1, 4, 8, 12, 24], window=36):
        """
        Args:
            df: DataFrame ƒë√£ ƒë∆∞·ª£c scale
            df_original: DataFrame G·ªêC (ch∆∞a scale) ƒë·ªÉ l·∫•y gi√° th·ª±c
            feature_cols: C√°c c·ªôt feature (bao g·ªìm pct_change)
            horizons: C√°c horizon c·∫ßn d·ª± ƒëo√°n
            window: S·ªë ng√†y lookback (L)
        """
        self.samples = []
        self.horizons = horizons

        features = df[feature_cols].values
        close_original = df_original['close'].values  # Gi√° G·ªêC ch∆∞a scale

        n = len(df)
        for i in range(n - window - max(horizons)):
            X = features[i : i + window]

            # Base price G·ªêC (ng√†y cu·ªëi window)
            base_price = close_original[i + window - 1]

            # Target: cumulative pct_change t·ª´ ng√†y L ƒë·∫øn ng√†y L+H
            y = []
            actual_prices = []
            for h in horizons:
                future_price = close_original[i + window + h - 1]
                cumulative_return = (future_price - base_price) / base_price if base_price != 0 else 0
                y.append(cumulative_return)
                actual_prices.append(future_price)

            self.samples.append((X, y, base_price, actual_prices))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        X, y, base_price, actual_prices = self.samples[idx]
        return (torch.tensor(X, dtype=torch.float32),
                torch.tensor(y, dtype=torch.float32),
                torch.tensor(base_price, dtype=torch.float32),
                torch.tensor(actual_prices, dtype=torch.float32))

"""#Model LSTM"""

class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2,
                 num_horizons=5, dropout=0.3):
        super(LSTM, self).__init__()

        self.encoder = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_horizons)
        )

    def forward(self, x):
        out, _ = self.encoder(x)
        context = out[:, -1, :]
        return self.fc(context)

"""#EarlyStopping class"""

# Early Stopping
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.best = None
        self.count = 0
        self.best_state = None

    def step(self, metric, model):
        improved = (self.best is None) or (metric < self.best - self.min_delta)
        if improved:
            self.best = metric
            self.count = 0
            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            self.count += 1
        return improved, self.count >= self.patience

"""#Optimize Hyperparameters"""

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

H_LIST = [1, 4, 7, 10, 14, 21]

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

def objective(trial):
    # Hyperparameters c·∫ßn t·ªëi ∆∞u
    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128, 256])
    num_layers = trial.suggest_int('num_layers', 1, 3)
    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)
    L = trial.suggest_categorical('L', [12, 24, 36, 48, 60, 72, 84, 96])

    # T·∫°o dataset
    train_dataset = StockDataset(train_df, train_df_original, feature_cols, horizons=H_LIST, window=L)
    valid_dataset = StockDataset(valid_df, valid_df_original, feature_cols, horizons=H_LIST, window=L)

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        raise optuna.TrialPruned()

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    # T·∫°o model
    model = LSTM(
        input_dim=len(feature_cols),
        hidden_dim=hidden_dim,
        num_layers=num_layers,
        num_horizons=len(H_LIST),
        dropout=dropout
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    # Training loop
    for epoch in range(1, 51):
        model.train()
        for X_batch, y_batch, _, _ in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            loss = criterion(model(X_batch), y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch, _, _ in valid_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                val_loss += criterion(model(X_batch), y_batch).item()
        val_loss /= len(valid_loader)

        # Pruning
        trial.report(val_loss, epoch)
        if trial.should_prune():
            raise optuna.TrialPruned()

        improved, should_stop = early_stopping.step(val_loss, model)
        if should_stop:
            break

    return early_stopping.best

study = optuna.create_study(
    direction='minimize',
    sampler=TPESampler(seed=42),
    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)
)

study.optimize(objective, n_trials=50, show_progress_bar=True)

# K·∫øt qu·∫£
print(f"\n{'='*50}")
print(f"BEST PARAMS:")
print(f"  hidden_dim: {study.best_params['hidden_dim']}")
print(f"  num_layers: {study.best_params['num_layers']}")
print(f"  dropout: {study.best_params['dropout']}")
print(f"  L: {study.best_params['L']}")
print(f"  Valid Loss: {study.best_value:.6f}")

"""#Training Loops, kh·ªüi t·∫°o dataset, dataloader, model v√† t√¨m L t·ªët nh·∫•t"""

L_VALUES = [12, 24, 36, 48, 60, 72, 84, 96]
H_LIST = [1, 4, 7, 10, 14, 21]

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

for L in L_VALUES:
    print(f"\n{'='*60}")
    print(f"TRAINING WITH L = {L}")
    print(f"{'='*60}")

    train_dataset = StockDataset(train_df, train_df_original, feature_cols, horizons=H_LIST, window=L)
    valid_dataset = StockDataset(valid_df, valid_df_original, feature_cols, horizons=H_LIST, window=L)

    print(f"Train samples: {len(train_dataset)}, Valid samples: {len(valid_dataset)}")

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        print(f"Skipping L={L}: Not enough samples")
        continue

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    model = LSTM(
        input_dim=len(feature_cols),
        hidden_dim=32, #32, 1, 0.5
        num_layers=3,   #64, 2, 0.3 / 32, 3, 0.1
        num_horizons=len(H_LIST),
        dropout=0.1
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    EPOCHS = 100
    train_losses, valid_losses = [], []

    for epoch in range(1, EPOCHS + 1):
        # Training
        model.train()
        running_train = 0.0
        for X_batch, y_batch, _, _ in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()
            preds = model(X_batch)
            loss = criterion(preds, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            running_train += loss.item()

        epoch_train_loss = running_train / len(train_loader)
        train_losses.append(epoch_train_loss)

        # Validation
        model.eval()
        running_valid = 0.0
        with torch.no_grad():
            for X_batch, y_batch, _, _ in valid_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device)
                preds = model(X_batch)
                loss = criterion(preds, y_batch)
                running_valid += loss.item()

        epoch_valid_loss = running_valid / len(valid_loader)
        valid_losses.append(epoch_valid_loss)

        scheduler.step(epoch_valid_loss)

        if epoch % 10 == 0:
            print(f"Epoch {epoch:3d} | Train: {epoch_train_loss:.6f} | Valid: {epoch_valid_loss:.6f}")

        improved, should_stop = early_stopping.step(epoch_valid_loss, model)
        if should_stop:
            print(f"Early stopping at epoch {epoch}")
            break

    if early_stopping.best_state is not None:
        model.load_state_dict(early_stopping.best_state)

    final_valid_loss = early_stopping.best
    results_by_L[L] = {
        'valid_loss': final_valid_loss,
        'train_losses': train_losses,
        'valid_losses': valid_losses,
        'epochs_trained': len(train_losses)
    }

    print(f"L={L} -> Best Valid Loss: {final_valid_loss:.6f}")

    if final_valid_loss < best_valid_loss:
        best_valid_loss = final_valid_loss
        best_L = L
        best_model_state = early_stopping.best_state.copy()

print(f"\n{'='*60}")
print(f"BEST L = {best_L} with Valid Loss = {best_valid_loss:.6f}")
print(f"{'='*60}")

"""#Plot"""

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: So s√°nh valid loss c√°c L
L_list = list(results_by_L.keys())
valid_losses_list = [results_by_L[L]['valid_loss'] for L in L_list]

axes[0].bar(range(len(L_list)), valid_losses_list, color='steelblue')
axes[0].set_xticks(range(len(L_list)))
axes[0].set_xticklabels([f'L={L}' for L in L_list])
axes[0].set_ylabel('Best Valid Loss')
axes[0].set_title('Validation Loss by Window Size (L)')
axes[0].axhline(y=best_valid_loss, color='red', linestyle='--', label=f'Best: L={best_L}')
axes[0].legend()

# Plot 2: Training curves for best L
best_train_losses = results_by_L[best_L]['train_losses']
best_valid_losses_plot = results_by_L[best_L]['valid_losses']
epochs_trained = results_by_L[best_L]['epochs_trained']

axes[1].plot(range(1, epochs_trained + 1), best_train_losses, 'b-', label='Train Loss')
axes[1].plot(range(1, epochs_trained + 1), best_valid_losses_plot, 'r-', label='Valid Loss')

axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss (MSE)')
axes[1].set_title(f'Training Curves for Best L = {best_L}')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""#ƒê√ÅNH GI√Å TR√äN Test SET V·ªöI BEST L"""

print(f"\n{'='*60}")
print(f"EVALUATING ON TEST SET WITH BEST L = {best_L}")
print(f"{'='*60}")

# T·∫°o test dataset
test_dataset = StockDataset(
    test_df, test_df_original, feature_cols,
    horizons=H_LIST, window=best_L
)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"Test samples: {len(test_dataset)}")

# Load best model
model = LSTM(
    input_dim=len(feature_cols),
    hidden_dim=32,
    num_layers=3,
    num_horizons=len(H_LIST),
    dropout=0.1
).to(device)
model.load_state_dict(best_model_state)
model.eval()

# Collect predictions
all_y_true = []
all_y_pred = []
all_base_prices = []
all_actual_prices = []

with torch.no_grad():
    for X_batch, y_batch, base_prices, actual_prices in test_loader:
        X_batch = X_batch.to(device)
        preds = model(X_batch).cpu().numpy()

        all_y_true.append(y_batch.numpy())
        all_y_pred.append(preds)
        all_base_prices.append(base_prices.numpy())
        all_actual_prices.append(actual_prices.numpy())

all_y_true = np.concatenate(all_y_true, axis=0)
all_y_pred = np.concatenate(all_y_pred, axis=0)
all_base_prices = np.concatenate(all_base_prices, axis=0)
all_actual_prices = np.concatenate(all_actual_prices, axis=0)

"""#D·ª∞ ƒêO√ÅN TH·ª¨"""

print("\n=== TEST SET RESULTS (ACTUAL PRICE - VND) ===")

# Chuy·ªÉn pct_change v·ªÅ gi√°: close_H = close_L + (close_L * pct_change)
pred_prices = np.zeros_like(all_y_pred)
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_y_pred[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

for i, h in enumerate(H_LIST):
    actual = all_actual_prices[:, i]
    pred   = pred_prices[:, i]

    # Mean price (d√πng ƒë·ªÉ scale v·ªÅ 0‚Äì1)
    mean_price = actual.mean()

    # Metrics g·ªëc (VND)
    mae  = mean_absolute_error(actual, pred)            # VND
    rmse = np.sqrt(mean_squared_error(actual, pred))    # VND
    mape = np.mean(np.abs((actual - pred) / actual)) * 100
    r2   = r2_score(actual, pred)

    # Metrics normalized (0 ‚Üí 1)
    mae_norm  = mae / mean_price
    rmse_norm = rmse / mean_price

    # Print t·∫•t c·∫£ 6 metrics
    print(
        f"H={h:3d}: "
        f"MAE={mae:,.2f} VND | "
        f"RMSE={rmse:,.2f} VND | "
        f"MAE_norm={mae_norm:.5f} | "
        f"RMSE_norm={rmse_norm:.5f} | "
        f"MAPE={mape:.2f}% | "
        f"R2={r2:.5f}"
    )

print("\n" + "="*70)
print("SAMPLE PREDICTIONS (Last 5 samples)")
print("="*70)

for sample_idx in range(-5, 0):
    base = all_base_prices[sample_idx]
    print(f"\nüìä Sample {sample_idx}: Base Price = {base:,.0f} VND")
    print("-" * 60)

    for i, h in enumerate(H_LIST):
        actual_price = all_actual_prices[sample_idx, i]
        pred_pct = all_y_pred[sample_idx, i]

        # T√≠nh gi√° d·ª± ƒëo√°n: base + (base * pct_change)
        delta = base * pred_pct
        pred_price = base + delta

        error = abs(actual_price - pred_price)
        error_pct = error / actual_price * 100

        direction = "üìà" if pred_pct > 0 else "üìâ"

        print(f"  H={h:3d}: pct_change={pred_pct:+.4f} ({pred_pct*100:+.2f}%) {direction}")
        print(f"         Delta={delta:+,.0f} VND")
        print(f"         Actual={actual_price:>10,.0f} VND | Pred={pred_price:>10,.0f} VND | Error={error:>8,.0f} VND ({error_pct:.2f}%)")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, h in enumerate(H_LIST):
    if i >= len(axes):
        break

    actual = all_actual_prices[:, i]
    pred = pred_prices[:, i]

    axes[i].scatter(actual, pred, alpha=0.5, s=20)

    # ƒê∆∞·ªùng perfect prediction
    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')

    axes[i].set_xlabel('Actual Price (VND)')
    axes[i].set_ylabel('Predicted Price (VND)')
    axes[i].set_title(f'Horizon H={h}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

# ·∫®n subplot th·ª´a
if len(H_LIST) < len(axes):
    axes[-1].axis('off')

plt.suptitle('Actual vs Predicted Prices', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Plot predictions vs actual cho m·ªôt v√†i horizons
fig, axes = plt.subplots(len(H_LIST), 1, figsize=(14, 3*len(H_LIST)))

for i, h in enumerate(H_LIST):
    ax = axes[i] if len(H_LIST) > 1 else axes

    pred_prices = all_base_prices * (1 + all_y_pred[:, i])
    actual_prices = all_actual_prices[:, i]

    ax.plot(actual_prices, label='Actual', alpha=0.7)
    ax.plot(pred_prices, label='Predicted', alpha=0.7)
    ax.set_title(f'Horizon H={h}')
    ax.set_xlabel('Sample')
    ax.set_ylabel('Price')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""#Save model"""

torch.save({
    'model_state_dict': best_model_state,
    'best_L': best_L,
    'scaler_robust': scaler_robust,
    'scaler_close': scaler_close,
    'features_robust': features_robust,
    'feature_cols': feature_cols,
    'horizons': H_LIST
}, f'best_lstm_L{best_L}.pth')

print(f"\nModel saved as 'best_lstm_L{best_L}.pth'")