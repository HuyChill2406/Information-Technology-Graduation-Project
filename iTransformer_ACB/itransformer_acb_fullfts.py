# -*- coding: utf-8 -*-
"""iTransformer_ACB_Fullfts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WncXCYqx-g-4fJGD9B8iKUmUxbHeVGn0

# iTransformer Stock Prediction - Custom Implementation
## Based on thuml/iTransformer (ICLR 2024 Spotlight)

Paper: "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting"

**Key Idea**: iTransformer inverts the structure of vanilla Transformer:
- Each **variate (feature)** becomes a token instead of each time step
- **Self-attention** captures multivariate correlations across variates
- **Feed-forward network** learns series representations for each variate

This notebook implements iTransformer from scratch following the official implementation.

# Install and Import Libraries
"""

!pip install -q torch einops optuna

import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler

import random, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import Dataset, DataLoader
import math

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# Load Dataset"""

dataset = pd.read_csv("dataset_ACB_fullFeatures.csv")
dataset['date'] = pd.to_datetime(dataset['date'])
dataset.set_index('date', inplace=True)

dataset.head()

dataset.drop(columns=['ticker', 'field'], inplace=True)

numeric_vars = ['open', 'high', 'low', 'close','volume','RSI_14',
                'MA_10','MA_50','MA_200','MACD','MACD_SIGNAL','MACD_HIST',
                'ADX_14','STOCH_K','STOCH_D','STOCHRSI_14',
                'GDP √ó 10^9 (USD)','CPI','usd_vnd']
for var in numeric_vars:
    dataset[var] = pd.to_numeric(dataset[var], errors='coerce')

dataset.shape, dataset.info()

dataset.head()

"""# Data Preparation & Scaling"""

data = dataset.copy()
# L∆∞u data g·ªëc TR∆Ø·ªöC KHI SCALE
data_original = data.copy()

train_ratio = 0.6
valid_ratio = 0.15

n = len(data)
train_end = int(train_ratio * n)
valid_end = int((train_ratio + valid_ratio) * n)

train_df = data.iloc[:train_end].copy()
valid_df = data.iloc[train_end:valid_end].copy()
test_df  = data.iloc[valid_end:].copy()

# Data g·ªëc cho evaluation
train_df_original = data_original.iloc[:train_end].copy()
valid_df_original = data_original.iloc[train_end:valid_end].copy()
test_df_original  = data_original.iloc[valid_end:].copy()

print(f"Total: {n}")
print(f"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}")

# Features d√πng RobustScaler (KH√îNG bao g·ªìm close v√† pct_change)
features_robust = ['open', 'high', 'low','volume','RSI_14',
                'MA_10','MA_50','MA_200','MACD','MACD_SIGNAL','MACD_HIST',
                'ADX_14','STOCH_K','STOCH_D','STOCHRSI_14',
                'GDP √ó 10^9 (USD)','CPI','usd_vnd']

# RobustScaler cho c√°c features kh√°c
scaler_robust = RobustScaler().fit(train_df[features_robust])

train_df[features_robust] = scaler_robust.transform(train_df[features_robust])
valid_df[features_robust] = scaler_robust.transform(valid_df[features_robust])
test_df[features_robust]  = scaler_robust.transform(test_df[features_robust])

# StandardScaler cho close
scaler_close = StandardScaler().fit(train_df[['close']])

train_df['close'] = scaler_close.transform(train_df[['close']])
valid_df['close'] = scaler_close.transform(valid_df[['close']])
test_df['close']  = scaler_close.transform(test_df[['close']])

# KH√îNG scale pct_change - gi·ªØ nguy√™n
print(f"\nScaling summary:")
print(f"  RobustScaler: {features_robust}")
print(f"  StandardScaler: ['close']")

"""# Dataset Class"""

class StockDataset(Dataset):
    def __init__(self, df, df_original, feature_cols, horizons=[1, 4, 8, 12, 24], window=36):
        """
        Args:
            df: DataFrame ƒë√£ ƒë∆∞·ª£c scale
            df_original: DataFrame G·ªêC (ch∆∞a scale) ƒë·ªÉ l·∫•y gi√° th·ª±c
            feature_cols: C√°c c·ªôt feature (bao g·ªìm pct_change)
            horizons: C√°c horizon c·∫ßn d·ª± ƒëo√°n
            window: S·ªë ng√†y lookback (L)
        """
        self.samples = []
        self.horizons = horizons

        features = df[feature_cols].values
        close_original = df_original['close'].values  # Gi√° G·ªêC ch∆∞a scale

        n = len(df)
        for i in range(n - window - max(horizons)):
            X = features[i : i + window]

            # Base price G·ªêC (ng√†y cu·ªëi window)
            base_price = close_original[i + window - 1]

            # Target: cumulative pct_change t·ª´ ng√†y L ƒë·∫øn ng√†y L+H
            y = []
            actual_prices = []
            for h in horizons:
                future_price = close_original[i + window + h - 1]
                cumulative_return = (future_price - base_price) / base_price if base_price != 0 else 0
                y.append(cumulative_return)
                actual_prices.append(future_price)

            self.samples.append((X, y, base_price, actual_prices))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        X, y, base_price, actual_prices = self.samples[idx]
        return (torch.tensor(X, dtype=torch.float32),
                torch.tensor(y, dtype=torch.float32),
                torch.tensor(base_price, dtype=torch.float32),
                torch.tensor(actual_prices, dtype=torch.float32))

"""# iTransformer Model - Custom Implementation

## Architecture Overview:
1. **DataEmbedding_inverted**: Project each variate's time series to d_model dimensions
2. **Encoder**: Standard Transformer encoder but applied on inverted dimensions
   - Self-attention captures multivariate correlations
   - FFN learns series representations
3. **Projection**: Output head for regression
"""

class DataEmbedding_inverted(nn.Module):
    """
    Inverted Embedding: Project each variate's lookback sequence to d_model
    Input: [batch, seq_len, num_variates]
    Output: [batch, num_variates, d_model]
    """
    def __init__(self, seq_len, d_model, dropout=0.1):
        super(DataEmbedding_inverted, self).__init__()
        self.value_embedding = nn.Linear(seq_len, d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        # x: [batch, seq_len, num_variates]
        # Transpose to [batch, num_variates, seq_len]
        x = x.permute(0, 2, 1)
        # Project seq_len -> d_model: [batch, num_variates, d_model]
        x = self.value_embedding(x)
        return self.dropout(x)

class FullAttention(nn.Module):
    """Standard Multi-Head Self-Attention"""
    def __init__(self, d_model, n_heads, dropout=0.1):
        super(FullAttention, self).__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)

    def forward(self, queries, keys, values):
        batch_size = queries.size(0)

        # Linear projections
        Q = self.W_Q(queries)
        K = self.W_K(keys)
        V = self.W_V(values)

        # Reshape to [batch, n_heads, seq_len, d_k]
        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        # Apply attention to values
        context = torch.matmul(attn, V)

        # Reshape back
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)

        return self.W_O(context)

class EncoderLayer(nn.Module):
    """Transformer Encoder Layer"""
    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model

        self.attention = FullAttention(d_model, n_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Self-attention with residual
        attn_out = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_out))

        # FFN with residual
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)

        return x

class Encoder(nn.Module):
    """Stack of Encoder Layers"""
    def __init__(self, encoder_layers):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList(encoder_layers)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

class iTransformer(nn.Module):
    """
    iTransformer: Inverted Transformer for Time Series Forecasting

    Key differences from vanilla Transformer:
    - Each variate becomes a token (not each time step)
    - Attention captures multivariate correlations
    - FFN learns series representations

    Args:
        seq_len: Lookback length (L)
        num_variates: Number of input variates/features
        num_targets: Number of output targets (horizons)
        d_model: Model dimension
        n_heads: Number of attention heads
        e_layers: Number of encoder layers
        d_ff: Feed-forward dimension
        dropout: Dropout rate
    """
    def __init__(self, seq_len, num_variates, num_targets,
                 d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.3):
        super(iTransformer, self).__init__()

        self.seq_len = seq_len
        self.num_variates = num_variates

        # Inverted Embedding: project each variate's time series
        self.enc_embedding = DataEmbedding_inverted(seq_len, d_model, dropout)

        # Encoder
        self.encoder = Encoder([
            EncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(e_layers)
        ])

        # Output projection head for regression
        # Aggregate all variate representations and project to targets
        self.projection = nn.Sequential(
            nn.Linear(d_model * num_variates, d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, num_targets)
        )

    def forward(self, x):
        """
        Args:
            x: [batch, seq_len, num_variates] - Input time series
        Returns:
            out: [batch, num_targets] - Predicted cumulative returns
        """
        # Embedding: [batch, seq_len, num_variates] -> [batch, num_variates, d_model]
        enc_out = self.enc_embedding(x)

        # Encoder: attention across variates
        enc_out = self.encoder(enc_out)
        # enc_out: [batch, num_variates, d_model]

        # Flatten and project to targets
        enc_out = enc_out.reshape(enc_out.size(0), -1)  # [batch, num_variates * d_model]
        out = self.projection(enc_out)  # [batch, num_targets]

        return out

"""# Early Stopping"""

class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.best = None
        self.count = 0
        self.best_state = None

    def step(self, metric, model):
        improved = (self.best is None) or (metric < self.best - self.min_delta)
        if improved:
            self.best = metric
            self.count = 0
            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            self.count += 1
        return improved, self.count >= self.patience

"""#Optimize HyperParameters"""

# Configuration
H_LIST = [1, 4, 7, 10, 14, 21]       # Horizons to predict

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

def objective(trial):
    # === HYPERPARAMETERS C·∫¶N T·ªêI ∆ØU ===
    L = trial.suggest_categorical('L', [12, 24, 36, 48, 60, 72, 84, 96])
    d_model = trial.suggest_categorical('d_model', [32, 64, 128, 256])
    n_heads = trial.suggest_categorical('n_heads', [2, 4, 8])
    e_layers = trial.suggest_int('e_layers', 1, 4)
    d_ff = trial.suggest_categorical('d_ff', [32, 64, 128, 256, 512])
    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)

    # === R√ÄNG BU·ªòC ===
    # d_model ph·∫£i chia h·∫øt cho n_heads
    if d_model % n_heads != 0:
        raise optuna.TrialPruned()

    # === T·∫†O DATASET ===
    train_dataset = StockDataset(train_df, train_df_original, feature_cols, horizons=H_LIST, window=L)
    valid_dataset = StockDataset(valid_df, valid_df_original, feature_cols, horizons=H_LIST, window=L)

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        raise optuna.TrialPruned()

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    # === T·∫†O MODEL ===
    num_features = len(feature_cols)

    model = iTransformer(
        seq_len=L,
        num_variates=num_features,
        num_targets=len(H_LIST),
        d_model=d_model,
        n_heads=n_heads,
        e_layers=e_layers,
        d_ff=d_ff,
        dropout=dropout
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    # === TRAINING LOOP ===
    for epoch in range(1, 51):
        model.train()
        for X_batch, y_batch, _, _ in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            preds = model(X_batch)
            loss = criterion(preds, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for X_batch, y_batch, _, _ in valid_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                preds = model(X_batch)
                val_loss += criterion(preds, y_batch).item()
        val_loss /= len(valid_loader)

        # Pruning
        trial.report(val_loss, epoch)
        if trial.should_prune():
            raise optuna.TrialPruned()

        improved, should_stop = early_stopping.step(val_loss, model)
        if should_stop:
            break

    return early_stopping.best

study = optuna.create_study(
    direction='minimize',
    sampler=TPESampler(seed=42),
    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=5)
)

study.optimize(objective, n_trials=50, show_progress_bar=True)

# K·∫øt qu·∫£
print(f"\n{'='*50}")
print(f"BEST PARAMS:")
print(f"  L: {study.best_params['L']}")
print(f"  d_model: {study.best_params['d_model']}")
print(f"  n_heads: {study.best_params['n_heads']}")
print(f"  e_layers: {study.best_params['e_layers']}")
print(f"  d_ff: {study.best_params['d_ff']}")
print(f"  dropout: {study.best_params['dropout']}")
print(f"  Valid Loss: {study.best_value:.6f}")

"""# Training iTransformer"""

# === S·ª¨ D·ª§NG BEST PARAMS T·ª™ OPTUNA (tr·ª´ L) ===
best_d_model = study.best_params['d_model']
best_n_heads = study.best_params['n_heads']
best_e_layers = study.best_params['e_layers']
best_d_ff = study.best_params['d_ff']
best_dropout = study.best_params['dropout']

print(f"Using Optuna best params:")
print(f"  d_model={best_d_model}, n_heads={best_n_heads}")
print(f"  e_layers={best_e_layers}, d_ff={best_d_ff}, dropout={best_dropout}")

# Configuration
L_VALUES = [12, 24, 36, 48, 60, 72, 84, 96]  # Context length (window size)
H_LIST = [1, 4, 7, 10, 14, 21]       # Horizons to predict

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

for L in L_VALUES:
    print(f"\n{'='*60}")
    print(f"TRAINING iTransformer (Custom) WITH L = {L}")
    print(f"{'='*60}")

    # Create dataset
    train_dataset = StockDataset(train_df, train_df_original, feature_cols, horizons=H_LIST, window=L)
    valid_dataset = StockDataset(valid_df, valid_df_original, feature_cols, horizons=H_LIST, window=L)

    print(f"Train samples: {len(train_dataset)}, Valid samples: {len(valid_dataset)}")

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        print(f"Skipping L={L}: Not enough samples")
        continue

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    # Model configuration v·ªõi params t·ª´ Optuna
    num_features = len(feature_cols)

    model = iTransformer(
        seq_len=L,
        num_variates=num_features,
        num_targets=len(H_LIST),
        d_model=best_d_model,
        n_heads=best_n_heads,
        e_layers=best_e_layers,
        d_ff=best_d_ff,
        dropout=best_dropout
    ).to(device)

    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"iTransformer parameters: {num_params:,}")

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    EPOCHS = 50
    train_losses, valid_losses = [], []

    for epoch in range(1, EPOCHS + 1):
        # TRAINING
        model.train()
        running_train = 0.0
        for X_batch, y_batch, _, _ in train_loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()
            preds = model(X_batch)
            loss = criterion(preds, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_train += loss.item() * X_batch.size(0)

        avg_train = running_train / len(train_dataset)
        train_losses.append(avg_train)

        # VALIDATION
        model.eval()
        running_valid = 0.0
        with torch.no_grad():
            for X_batch, y_batch, _, _ in valid_loader:
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device)
                preds = model(X_batch)
                loss = criterion(preds, y_batch)
                running_valid += loss.item() * X_batch.size(0)

        avg_valid = running_valid / len(valid_dataset)
        valid_losses.append(avg_valid)

        scheduler.step(avg_valid)
        improved, stop = early_stopping.step(avg_valid, model)

        if epoch % 10 == 0 or improved:
            mark = "*" if improved else ""
            print(f"Epoch {epoch:3d} | Train: {avg_train:.6f} | Valid: {avg_valid:.6f} {mark}")

        if stop:
            print(f"Early stopping at epoch {epoch}")
            break

    final_valid_loss = early_stopping.best
    results_by_L[L] = {
        'valid_loss': final_valid_loss,
        'train_losses': train_losses,
        'valid_losses': valid_losses,
        'model_state': early_stopping.best_state
    }

    if final_valid_loss < best_valid_loss:
        best_valid_loss = final_valid_loss
        best_L = L
        best_model_state = early_stopping.best_state

    print(f"\nL={L} completed. Best valid loss: {final_valid_loss:.6f}")

print(f"\n{'='*60}")
print(f"BEST L = {best_L} with valid loss = {best_valid_loss:.6f}")
print(f"{'='*60}")

"""# Plot Training, Valid Loss"""

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: Compare valid loss across L values
L_list = list(results_by_L.keys())
valid_losses_list = [results_by_L[L]['valid_loss'] for L in L_list]

axes[0].bar(range(len(L_list)), valid_losses_list, color='steelblue')
axes[0].set_xticks(range(len(L_list)))
axes[0].set_xticklabels([f'L={L}' for L in L_list])
axes[0].set_ylabel('Best Valid Loss')
axes[0].set_title('Validation Loss by Window Size (L)')
axes[0].axhline(y=best_valid_loss, color='red', linestyle='--', label=f'Best: L={best_L}')
axes[0].legend()

# Plot 2: Training curves for best L
best_train_losses = results_by_L[best_L]['train_losses']
best_valid_losses_plot = results_by_L[best_L]['valid_losses']

axes[1].plot(range(1, len(best_train_losses) + 1), best_train_losses, 'b-', label='Train Loss')
axes[1].plot(range(1, len(best_valid_losses_plot) + 1), best_valid_losses_plot, 'r-', label='Valid Loss')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss (MSE)')
axes[1].set_title(f'Training Curves for Best L = {best_L}')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""# Evaluate on Test with Best L"""

print(f"\n{'='*60}")
print(f"EVALUATING ON TEST SET WITH BEST L = {best_L}")
print(f"{'='*60}")

# Create test dataset
test_dataset = StockDataset(
    test_df, test_df_original, feature_cols,
    horizons=H_LIST, window=best_L
)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"Test samples: {len(test_dataset)}")

# Recreate model with best L
num_features = len(feature_cols)

model = iTransformer(
    seq_len=best_L,
    num_variates=num_features,
    num_targets=len(H_LIST),
    d_model=best_d_model,
    n_heads=best_n_heads,
    e_layers=best_e_layers,
    d_ff=best_d_ff,
    dropout=best_dropout
    ).to(device)

model.load_state_dict(best_model_state)
model.eval()

# Collect predictions
all_preds = []
all_targets = []
all_base_prices = []
all_actual_prices = []

with torch.no_grad():
    for X_batch, y_batch, base_batch, actual_batch in test_loader:
        X_batch = X_batch.to(device)
        preds = model(X_batch)

        all_preds.append(preds.cpu().numpy())
        all_targets.append(y_batch.numpy())
        all_base_prices.append(base_batch.numpy())
        all_actual_prices.append(actual_batch.numpy())

all_preds = np.concatenate(all_preds, axis=0)
all_targets = np.concatenate(all_targets, axis=0)
all_base_prices = np.concatenate(all_base_prices, axis=0)
all_actual_prices = np.concatenate(all_actual_prices, axis=0)

print(f"Predictions shape: {all_preds.shape}")
print(f"Targets shape: {all_targets.shape}")

print("\n=== TEST SET RESULTS (ACTUAL PRICE - VND) ===")

# Convert cumulative return to price: pred_price = base_price * (1 + cumulative_return)
pred_prices = np.zeros_like(all_preds)
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_preds[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

for i, h in enumerate(H_LIST):
    actual = all_actual_prices[:, i]
    pred   = pred_prices[:, i]

    # Mean price (for normalization)
    mean_price = actual.mean()

    # Metrics (VND)
    mae  = mean_absolute_error(actual, pred)
    rmse = np.sqrt(mean_squared_error(actual, pred))
    mape = np.mean(np.abs((actual - pred) / actual)) * 100
    r2   = r2_score(actual, pred)

    # Normalized metrics (0 ‚Üí 1)
    mae_norm  = mae / mean_price
    rmse_norm = rmse / mean_price

    print(
        f"H={h:3d}: "
        f"MAE={mae:,.2f} VND | "
        f"RMSE={rmse:,.2f} VND | "
        f"MAE_norm={mae_norm:.5f} | "
        f"RMSE_norm={rmse_norm:.5f} | "
        f"MAPE={mape:.2f}% | "
        f"R2={r2:.5f}"
    )

"""# Visualize Predictions"""

print("\n" + "="*70)
print("SAMPLE PREDICTIONS (Last 5 samples)")
print("="*70)

for sample_idx in range(-5, 0):
    base = all_base_prices[sample_idx]
    print(f"\nüìä Sample {sample_idx}: Base Price = {base:,.0f} VND")
    print("-" * 60)

    for i, h in enumerate(H_LIST):
        actual_price = all_actual_prices[sample_idx, i]
        pred_pct = all_preds[sample_idx, i]

        # Calculate predicted price: base + (base * cumulative_return)
        delta = base * pred_pct
        pred_price = base + delta

        error = abs(actual_price - pred_price)
        error_pct = error / actual_price * 100

        direction = "üìà" if pred_pct > 0 else "üìâ"

        print(f"  H={h:3d}: cumulative_return={pred_pct:+.4f} ({pred_pct*100:+.2f}%) {direction}")
        print(f"         Delta={delta:+,.0f} VND")
        print(f"         Actual={actual_price:>10,.0f} VND | Pred={pred_price:>10,.0f} VND | Error={error:>8,.0f} VND ({error_pct:.2f}%)")

# T√≠nh pred_prices tr∆∞·ªõc khi plot
pred_prices = np.zeros_like(all_preds)  # [N, 5]
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_preds[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

# Ki·ªÉm tra shape
print(f"pred_prices shape: {pred_prices.shape}")
print(f"all_actual_prices shape: {all_actual_prices.shape}")

# Plot
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, h in enumerate(H_LIST):
    if i >= len(axes):
        break

    actual = all_actual_prices[:, i]
    pred = pred_prices[:, i]

    axes[i].scatter(actual, pred, alpha=0.5, s=20)

    # ƒê∆∞·ªùng perfect prediction
    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')

    axes[i].set_xlabel('Actual Price (VND)')
    axes[i].set_ylabel('Predicted Price (VND)')
    axes[i].set_title(f'Horizon H={h}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

# ·∫®n subplot th·ª´a
if len(H_LIST) < len(axes):
    axes[-1].axis('off')

plt.suptitle('Actual vs Predicted Prices', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Plot predictions vs actual cho m·ªôt v√†i horizons
fig, axes = plt.subplots(len(H_LIST), 1, figsize=(14, 3*len(H_LIST)))

for i, h in enumerate(H_LIST):
    ax = axes[i] if len(H_LIST) > 1 else axes

    pred_prices = all_base_prices * (1 + all_preds[:, i])
    actual_prices = all_actual_prices[:, i]

    ax.plot(actual_prices, label='Actual', alpha=0.7)
    ax.plot(pred_prices, label='Predicted', alpha=0.7)
    ax.set_title(f'Horizon H={h}')
    ax.set_xlabel('Sample')
    ax.set_ylabel('Price')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""# Save Model"""

torch.save({
    'model_state_dict': best_model_state,
    'best_L': best_L,
    'scaler_robust': scaler_robust,
    'scaler_close': scaler_close,
    'features_robust': features_robust,
    'feature_cols': feature_cols,
    'horizons': H_LIST,
    'model_type': 'iTransformer_Custom',
    'config': {
        'seq_len': best_L,
        'num_variates': len(feature_cols),
        'num_targets': len(H_LIST),
        'd_model': best_d_model,
        'n_heads': best_n_heads,
        'e_layers': best_e_layers,
        'd_ff': best_d_ff,
        'dropout': best_dropout,
    }
}, f'itransformer_Fullfts_L{best_L}.pth')

print(f"\nModel saved as 'itransformer_Fullfts_L{best_L}.pth'")