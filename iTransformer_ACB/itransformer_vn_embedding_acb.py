# -*- coding: utf-8 -*-
"""iTransformer_VN_Embedding_ACB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yzi5pK7bT6wIb0rinAdydGwraoNuOgCK

# iTransformer + AITeamVN/Vietnamese_Embedding Multimodal Stock Prediction

**Architecture:**
- **Text Encoder**: https://huggingface.co/AITeamVN/Vietnamese_Embedding - encode news text
- **Time Series Encoder**: iTransformer - inverted transformer for time series (inverted transformer - attention across variates)
- **Fusion**: Concatenate or Cross-Attention between embeddings from both encoders
- **Decoder**: Fully Connected layers - predict multiple horizons

**iTransformer Key Idea**: Each variate (feature) becomes a token instead of each time step
- Self-attention captures multivariate correlations across variates
- Feed-forward network learns series representations for each variate

**Text Selection Logic:**
- Use news from the most recent three days (H, H‚àí1, H‚àí2)

- Apply average pooling per day to aggregate multiple news articles published on the same day

- Use sinusoidal positional encoding to preserve temporal order

- Apply cross-attention, where queries come from the iTransformer output and keys/values come from the news representations

**iTransformer vs Standard Transformer:**
- Standard: Attention across time steps
- iTransformer: Attention across variates (features) - better for multivariate forecasting

# Install and Import Libraries
"""

!pip install -q torch transformers accelerate einops sentence-transformers

import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from datetime import timedelta

from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, root_mean_squared_error

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import Dataset, DataLoader

from sentence_transformers import SentenceTransformer

import warnings
warnings.filterwarnings('ignore')

import pickle

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

"""# Load Embedding Model"""

MODEL_NAME = "AITeamVN/Vietnamese_Embedding"

print(f"Loading {MODEL_NAME}...")
embedding_model = SentenceTransformer(MODEL_NAME)
embedding_model = embedding_model.to(device)
embedding_model.eval()

EMBEDDING_DIM = embedding_model.get_sentence_embedding_dimension()
print(f"Model loaded successfully!")
print(f"Embedding dimension: {EMBEDDING_DIM}")

# Freeze model parameters
FREEZE_MODEL = True

if FREEZE_MODEL:
    for param in embedding_model.parameters():
        param.requires_grad = False
    print("Embedding model parameters frozen")

"""# Load Datasets"""

# Load news data
news_df = pd.read_csv("merged_kaggle_collected_news.csv")
news_df['date'] = pd.to_datetime(news_df['date'])

# Sort by date
news_df = news_df.sort_values('date').reset_index(drop=True)

print(f"News data shape: {news_df.shape}")
print(f"Date range: {news_df['date'].min()} to {news_df['date'].max()}")
print(f"\nColumns: {news_df.columns.tolist()}")

news_df

# Check news per day distribution
news_per_day = news_df.groupby('date').size()
print(f"\nNews statistics:")
print(f"  Total unique dates with news: {len(news_per_day)}")
print(f"  Mean news per day: {news_per_day.mean():.2f}")
print(f"  Max news per day: {news_per_day.max()}")
print(f"  Min news per day: {news_per_day.min()}")

# Group news by date - keep as list of articles per date
news_by_date = news_df.groupby('date')['content_clean'].apply(list).to_dict()

print(f"Created news lookup for {len(news_by_date)} dates")
print(f"Sample dates: {list(news_by_date.keys())[:3]}")

# Load stock price data
dataset = pd.read_csv("dataset_ACB_fullFeatures.csv")

dataset['date'] = pd.to_datetime(dataset['date'])
dataset.set_index('date', inplace=True)

# Drop unnecessary columns if exist
cols_to_drop = ['ticker', 'field']
dataset.drop(columns=[c for c in cols_to_drop if c in dataset.columns], inplace=True)

print(f"Stock data shape: {dataset.shape}")
print(f"Date range: {dataset.index.min()} to {dataset.index.max()}")

numeric_vars = ['open', 'high', 'low','close','volume','RSI_14',
                'MA_10', 'MA_50', 'MA_200','MACD','MACD_SIGNAL','MACD_HIST',
                'ADX_14','STOCH_K','STOCH_D',
                'GDP √ó 10^9 (USD)', 'CPI', 'usd_vnd']
for var in numeric_vars:
    dataset[var] = pd.to_numeric(dataset[var], errors='coerce')

dataset.shape, dataset.info()

"""# Prepare Data"""

data = dataset.copy()

# L∆∞u data g·ªëc TR∆Ø·ªöC KHI SCALE
data_original = data.copy()

train_ratio = 0.6
valid_ratio = 0.15

n = len(data)
train_end = int(train_ratio * n)
valid_end = int((train_ratio + valid_ratio) * n)

train_df = data.iloc[:train_end].copy()
valid_df = data.iloc[train_end:valid_end].copy()
test_df  = data.iloc[valid_end:].copy()

# Data g·ªëc cho evaluation
train_df_original = data_original.iloc[:train_end].copy()
valid_df_original = data_original.iloc[train_end:valid_end].copy()
test_df_original  = data_original.iloc[valid_end:].copy()

print(f"Total: {n}")
print(f"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}")

# Features d√πng RobustScaler (KH√îNG bao g·ªìm close)
features_robust = ['open', 'high', 'low', 'volume', 'RSI_14',
                   'MA_10', 'MA_50', 'MA_200', 'MACD', 'MACD_SIGNAL', 'MACD_HIST',
                   'ADX_14', 'STOCH_K', 'STOCH_D', 'STOCHRSI_14',
                   'GDP √ó 10^9 (USD)', 'CPI', 'usd_vnd']

# RobustScaler cho c√°c features kh√°c
scaler_robust = RobustScaler().fit(train_df[features_robust])

train_df[features_robust] = scaler_robust.transform(train_df[features_robust])
valid_df[features_robust] = scaler_robust.transform(valid_df[features_robust])
test_df[features_robust]  = scaler_robust.transform(test_df[features_robust])

# StandardScaler cho close
scaler_close = StandardScaler().fit(train_df[['close']])

train_df['close'] = scaler_close.transform(train_df[['close']])
valid_df['close'] = scaler_close.transform(valid_df[['close']])
test_df['close']  = scaler_close.transform(test_df[['close']])

print(f"\nScaling summary:")
print(f"  RobustScaler: {features_robust}")
print(f"  StandardScaler: ['close']")

"""# Pre-encode Text with AITeamVN/Vietnamese_Embedding"""

def encode_text(text, model, device):
    """
    Encode a single text using SentenceTransformer.
    Returns the sentence embedding.
    """
    if not text or len(str(text).strip()) == 0:
        return torch.zeros(EMBEDDING_DIM)

    with torch.no_grad():
        embedding = model.encode(
            text,
            convert_to_tensor=True,
            device=device
        )

    return embedding.cpu()

# Test encoding
test_text = "Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam tƒÉng m·∫°nh trong phi√™n giao d·ªãch h√¥m nay."
test_embedding = encode_text(test_text, embedding_model, device)
print(f"Test embedding shape: {test_embedding.shape}")
print(test_embedding)

def encode_articles_with_average_pool(articles_list, model, device):
    """
    Encode multiple articles and return their average embedding.
    """
    if not articles_list or len(articles_list) == 0:
        return torch.zeros(EMBEDDING_DIM)

    valid_articles = [a for a in articles_list if a and len(str(a).strip()) > 0]

    if len(valid_articles) == 0:
        return torch.zeros(EMBEDDING_DIM)

    with torch.no_grad():
        embeddings = model.encode(
            valid_articles,
            convert_to_tensor=True,
            device=device,
            batch_size=32
        )
        avg_embedding = embeddings.mean(dim=0)

    return avg_embedding.cpu()

# Pre-encode all dates - using AVERAGE POOLING for multiple articles per day
print(f"Pre-encoding all news with average pooling using {MODEL_NAME}...")

text_embeddings_cache = {}

for i, (date, articles_list) in enumerate(news_by_date.items()):
    # Average pool all articles for this day
    avg_embedding = encode_articles_with_average_pool(articles_list, embedding_model, device)
    text_embeddings_cache[date] = avg_embedding

    if (i + 1) % 200 == 0:
        print(f"  Processed {i + 1}/{len(news_by_date)} dates")

print(f"\nPre-encoded {len(text_embeddings_cache)} date embeddings (with average pooling)")
print(f"Embedding dimension: {EMBEDDING_DIM}")

# Save cache sau khi encode xong
with open('text_embeddings_cache.pkl', 'wb') as f:
    pickle.dump(text_embeddings_cache, f)

# Load l·∫°i l·∫ßn sau (ch·ªâ v√†i gi√¢y)
with open('text_embeddings_cache.pkl', 'rb') as f:
    text_embeddings_cache = pickle.load(f)

"""#Sinusoidal Positional Encoding"""

class SinusoidalPositionalEncoding(nn.Module):
    """
    Sinusoidal Positional Encoding for temporal awareness.

    Cho ph√©p model ph√¢n bi·ªát tin t·ª©c ng√†y H (g·∫ßn nh·∫•t), H-1, H-2 (xa h∆°n).

    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
    """
    def __init__(self, d_model, max_len=10):
        super(SinusoidalPositionalEncoding, self).__init__()

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # Register as buffer (not a parameter, but saved with model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: Tensor of shape (batch_size, seq_len, d_model)

        Returns:
            x + positional encoding
        """
        seq_len = x.size(1)
        return x + self.pe[:seq_len, :].unsqueeze(0)

"""# Dataset Class with News"""

# Configuration for multi-day news
NEWS_LOOKBACK_DAYS = 3  # Use news from H, H-1, H-2

print(f"News lookback: {NEWS_LOOKBACK_DAYS} days")
print(f"For predicting H+1, H+4, ... will use news from days: H, H-1, H-2")

def get_multiday_news_embeddings(target_date, text_embeddings_cache,
                                  num_days=3, embedding_dim=768, max_search_days=30):
    """
    Get news embeddings for multiple days (H, H-1, H-2, ...).
    """
    if not isinstance(target_date, pd.Timestamp):
        target_date = pd.Timestamp(target_date)

    embeddings = []

    for day_offset in range(num_days):
        check_date = target_date - timedelta(days=day_offset)

        found_embedding = None
        for search_offset in range(max_search_days + 1):
            search_date = check_date - timedelta(days=search_offset)
            if search_date in text_embeddings_cache:
                found_embedding = text_embeddings_cache[search_date]
                break

        if found_embedding is None:
            found_embedding = torch.zeros(embedding_dim)

        embeddings.append(found_embedding)

    return torch.stack(embeddings, dim=0)

class StockMultiDayNewsDataset(Dataset):
    """
    Dataset with MULTI-DAY news embeddings.
    """
    def __init__(self, df, df_original, feature_cols,
                 text_embeddings_cache,
                 horizons=[1, 4, 8, 12, 24], window=36,
                 embedding_dim=768, num_news_days=3, max_lookback_days=30):

        self.samples = []
        self.horizons = horizons
        self.embedding_dim = embedding_dim
        self.num_news_days = num_news_days

        features = df[feature_cols].values
        close_original = df_original['close'].values
        dates = df.index

        n = len(df)
        for i in range(n - window - max(horizons)):
            X = features[i : i + window]
            last_date_in_window = dates[i + window - 1]

            text_emb = get_multiday_news_embeddings(
                last_date_in_window,
                text_embeddings_cache,
                num_days=num_news_days,
                embedding_dim=embedding_dim,
                max_search_days=max_lookback_days
            )

            base_price = close_original[i + window - 1]

            y = []
            actual_prices = []
            for h in horizons:
                future_price = close_original[i + window - 1 + h]
                cumulative_return = (future_price - base_price) / base_price
                y.append(cumulative_return)
                actual_prices.append(future_price)

            self.samples.append((
                torch.tensor(X, dtype=torch.float32),
                text_emb,
                torch.tensor(y, dtype=torch.float32),
                base_price,
                torch.tensor(actual_prices, dtype=torch.float32)
            ))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

"""# iTransformer Components"""

class DataEmbedding_inverted(nn.Module):
    """
    Inverted Embedding: Project each variate's lookback sequence to d_model.

    Key insight of iTransformer:
    - Standard transformer: attention across time steps
    - iTransformer: attention across variates (features)

    Input: [batch, seq_len, num_variates]
    Output: [batch, num_variates, d_model]
    """
    def __init__(self, seq_len, d_model, dropout=0.1):
        super(DataEmbedding_inverted, self).__init__()
        self.value_embedding = nn.Linear(seq_len, d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        # x: [batch, seq_len, num_variates]
        # Transpose to [batch, num_variates, seq_len]
        x = x.permute(0, 2, 1)
        # Project seq_len -> d_model: [batch, num_variates, d_model]
        x = self.value_embedding(x)
        return self.dropout(x)

class FullAttention(nn.Module):
    """Standard Multi-Head Self-Attention"""
    def __init__(self, d_model, n_heads, dropout=0.1):
        super(FullAttention, self).__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)

    def forward(self, queries, keys, values):
        batch_size = queries.size(0)

        # Linear projections
        Q = self.W_Q(queries)
        K = self.W_K(keys)
        V = self.W_V(values)

        # Reshape to [batch, n_heads, seq_len, d_k]
        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        # Attention scores
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        attn = F.softmax(scores, dim=-1)
        attn = self.dropout(attn)

        # Apply attention to values
        context = torch.matmul(attn, V)

        # Reshape back
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)

        return self.W_O(context)

class EncoderLayer(nn.Module):
    """Transformer Encoder Layer"""
    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model

        self.attention = FullAttention(d_model, n_heads, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # Self-attention with residual
        attn_out = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_out))

        # FFN with residual
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)

        return x

class Encoder(nn.Module):
    """Stack of Encoder Layers"""
    def __init__(self, encoder_layers):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList(encoder_layers)

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

"""# Multimodal Model: iTransformer + AITeamVN/Vietnamese_Embedding

##Fusion concatenate
"""

# Concat model (baseline)
class iTransformerConcatMultiDayNews(nn.Module):
    """
    iTransformer + Concat model (baseline comparison).
    """
    def __init__(self, seq_len, num_variates, num_targets,
                 d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.3,
                 embedding_dim=768, num_news_days=3,
                 text_proj_dim=64, use_text=True):
        super(iTransformerConcatMultiDayNews, self).__init__()

        self.use_text = use_text
        self.d_model = d_model
        self.num_variates = num_variates

        # iTransformer Backbone
        self.enc_embedding = DataEmbedding_inverted(seq_len, d_model, dropout)
        self.encoder = Encoder([
            EncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(e_layers)
        ])

        if use_text:
            self.positional_encoding = SinusoidalPositionalEncoding(
                d_model=embedding_dim,
                max_len=num_news_days + 5
            )

            self.text_projection = nn.Sequential(
                nn.Linear(embedding_dim, text_proj_dim * 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(text_proj_dim * 2, text_proj_dim),
                nn.GELU()
            )

            fusion_dim = d_model * num_variates + text_proj_dim
        else:
            fusion_dim = d_model * num_variates

        self.projection = nn.Sequential(
            nn.Linear(fusion_dim, d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, num_targets)
        )

    def forward(self, x_ts, x_text=None):
        batch_size = x_ts.size(0)

        enc_out = self.enc_embedding(x_ts)
        enc_out = self.encoder(enc_out)
        ts_embedding = enc_out.reshape(batch_size, -1)

        if self.use_text and x_text is not None:
            news_with_pos = self.positional_encoding(x_text)
            news_avg = news_with_pos.mean(dim=1)
            text_proj = self.text_projection(news_avg)
            fused = torch.cat([ts_embedding, text_proj], dim=-1)
        else:
            fused = ts_embedding

        predictions = self.projection(fused)
        return predictions, None

"""##Cross attention"""

class iTransformerCrossAttentionMultiDayNews(nn.Module):
    """
    iTransformer + Cross-Attention model v·ªõi Multi-Day News Integration.

    Pipeline:
    1. News Encoder: Multi-day news ‚Üí Positional Encoding ‚Üí FFN
    2. Time-Series Encoder: iTransformer (attention across variates)
    3. Cross-Attention: Query = iTransformer variates, Key = Value = News
    4. Fusion & Prediction: Concatenate and decode
    """
    def __init__(self, seq_len, num_variates, num_targets,
                 d_model=128, n_heads=4, e_layers=2, d_ff=256, dropout=0.3,
                 embedding_dim=768, num_news_days=3,
                 num_attention_heads=4, use_text=True):
        super(iTransformerCrossAttentionMultiDayNews, self).__init__()

        self.use_text = use_text
        self.d_model = d_model
        self.num_variates = num_variates
        self.num_news_days = num_news_days

        # ===== iTransformer Backbone =====
        self.enc_embedding = DataEmbedding_inverted(seq_len, d_model, dropout)
        self.encoder = Encoder([
            EncoderLayer(d_model, n_heads, d_ff, dropout)
            for _ in range(e_layers)
        ])

        if use_text:
            # ===== News Encoder =====
            self.positional_encoding = SinusoidalPositionalEncoding(
                d_model=embedding_dim,
                max_len=num_news_days + 5
            )

            self.news_ffn = nn.Sequential(
                nn.Linear(embedding_dim, d_model * 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model * 2, d_model),
                nn.GELU()
            )

            # ===== Cross-Attention =====
            # Query = iTransformer variates, Key = Value = News
            self.cross_attention = nn.MultiheadAttention(
                embed_dim=d_model,
                num_heads=num_attention_heads,
                dropout=dropout,
                batch_first=True
            )

            self.attn_layer_norm = nn.LayerNorm(d_model)

            fusion_dim = d_model * num_variates + d_model
        else:
            fusion_dim = d_model * num_variates

        # ===== Decoder =====
        self.projection = nn.Sequential(
            nn.Linear(fusion_dim, d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, num_targets)
        )

    def forward(self, x_ts, x_text=None):
        """
        Args:
            x_ts: (batch, seq_len, num_variates)
            x_text: (batch, num_news_days, embedding_dim)
        """
        batch_size = x_ts.size(0)

        # ===== iTransformer Encoding =====
        # Embedding: [batch, seq_len, num_variates] -> [batch, num_variates, d_model]
        enc_out = self.enc_embedding(x_ts)
        # Encoder: attention across variates
        enc_out = self.encoder(enc_out)  # [batch, num_variates, d_model]

        # Flatten for ts_embedding
        ts_embedding = enc_out.reshape(batch_size, -1)  # [batch, num_variates * d_model]

        attention_weights = None

        if self.use_text and x_text is not None:
            # ===== Encode News =====
            news_with_pos = self.positional_encoding(x_text)
            news_proj = self.news_ffn(news_with_pos)  # (batch, num_news_days, d_model)

            # ===== Cross-Attention =====
            # Query = iTransformer variates (batch, num_variates, d_model)
            # Key = Value = News (batch, num_news_days, d_model)
            cross_attn_out, attention_weights = self.cross_attention(
                query=enc_out,       # (batch, num_variates, d_model)
                key=news_proj,       # (batch, num_news_days, d_model)
                value=news_proj      # (batch, num_news_days, d_model)
            )
            # cross_attn_out: (batch, num_variates, d_model)

            cross_attn_out = self.attn_layer_norm(cross_attn_out + enc_out)
            cross_attn_embedding = cross_attn_out.mean(dim=1)  # (batch, d_model)

            fused = torch.cat([ts_embedding, cross_attn_embedding], dim=-1)
        else:
            fused = ts_embedding

        predictions = self.projection(fused)

        return predictions, attention_weights

"""#EarlyStopping class"""

class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.0001):
        self.patience = patience
        self.min_delta = min_delta
        self.best = None
        self.count = 0
        self.best_state = None

    def step(self, metric, model):
        improved = (self.best is None) or (metric < self.best - self.min_delta)
        if improved:
            self.best = metric
            self.count = 0
            self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            self.count += 1
        return improved, self.count >= self.patience

"""# Training Configuration"""

# Hyperparameters
L_VALUES = [12, 24, 36, 48, 60, 72, 84, 96]
H_LIST = [1, 4, 7, 10, 14, 21]

# iTransformer hyperparameters
D_MODEL = 32
N_HEADS = 8
E_LAYERS = 4
D_FF = 128
DROPOUT = 0.3

# Model configuration
USE_TEXT = True
USE_CROSS_ATTENTION = True  # True = Cross-Attention, False = Concat

print(f"\n" + "="*60)
print("CONFIGURATION")
print("="*60)
print(f"  L values: {L_VALUES}")
print(f"  Horizons: {H_LIST}")
print(f"  D_MODEL: {D_MODEL}, N_HEADS: {N_HEADS}, E_LAYERS: {E_LAYERS}")
print(f"  Use text: {USE_TEXT}")
print(f"  Use cross-attention: {USE_CROSS_ATTENTION}")
print(f"  News lookback days: {NEWS_LOOKBACK_DAYS}")

feature_cols = features_robust + ['close']
print(f"\nFeature columns ({len(feature_cols)}): {feature_cols}")

best_L = None
best_valid_loss = float('inf')
best_model_state = None
results_by_L = {}

"""# Training Loop

##Fusion concatenate
"""

for L in L_VALUES:
    print(f"\n{'='*60}")
    print(f"TRAINING iTransformer WITH L = {L}")
    print(f"{'='*60}")

    train_dataset = StockMultiDayNewsDataset(
        train_df, train_df_original, feature_cols,
        text_embeddings_cache,
        horizons=H_LIST, window=L,
        embedding_dim=EMBEDDING_DIM,
        num_news_days=NEWS_LOOKBACK_DAYS
    )
    valid_dataset = StockMultiDayNewsDataset(
        valid_df, valid_df_original, feature_cols,
        text_embeddings_cache,
        horizons=H_LIST, window=L,
        embedding_dim=EMBEDDING_DIM,
        num_news_days=NEWS_LOOKBACK_DAYS
    )

    print(f"Train samples: {len(train_dataset)}, Valid samples: {len(valid_dataset)}")

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        print(f"Skipping L={L}: Not enough samples")
        continue

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    num_features = len(feature_cols)

    if USE_CROSS_ATTENTION:
        model = iTransformerCrossAttentionMultiDayNews(
            seq_len=L,
            num_variates=num_features,
            num_targets=len(H_LIST),
            d_model=D_MODEL,
            n_heads=N_HEADS,
            e_layers=E_LAYERS,
            d_ff=D_FF,
            dropout=DROPOUT,
            embedding_dim=EMBEDDING_DIM,
            num_news_days=NEWS_LOOKBACK_DAYS,
            num_attention_heads=4,
            use_text=USE_TEXT
        ).to(device)
    else:
        model = iTransformerConcatMultiDayNews(
            seq_len=L,
            num_variates=num_features,
            num_targets=len(H_LIST),
            d_model=D_MODEL,
            n_heads=N_HEADS,
            e_layers=E_LAYERS,
            d_ff=D_FF,
            dropout=DROPOUT,
            embedding_dim=EMBEDDING_DIM,
            num_news_days=NEWS_LOOKBACK_DAYS,
            text_proj_dim=64,
            use_text=USE_TEXT
        ).to(device)

    print(f"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    EPOCHS = 100
    train_losses, valid_losses = [], []

    for epoch in range(1, EPOCHS + 1):
        model.train()
        running_train = 0.0
        for X_batch, text_batch, y_batch, _, _ in train_loader:
            X_batch = X_batch.to(device)
            text_batch = text_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()

            if USE_TEXT:
                preds, _ = model(X_batch, text_batch)
            else:
                preds, _ = model(X_batch)

            loss = criterion(preds, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_train += loss.item() * X_batch.size(0)

        train_loss = running_train / len(train_dataset)
        train_losses.append(train_loss)

        model.eval()
        running_valid = 0.0
        with torch.no_grad():
            for X_batch, text_batch, y_batch, _, _ in valid_loader:
                X_batch = X_batch.to(device)
                text_batch = text_batch.to(device)
                y_batch = y_batch.to(device)

                if USE_TEXT:
                    preds, _ = model(X_batch, text_batch)
                else:
                    preds, _ = model(X_batch)

                loss = criterion(preds, y_batch)
                running_valid += loss.item() * X_batch.size(0)

        valid_loss = running_valid / len(valid_dataset)
        valid_losses.append(valid_loss)

        scheduler.step(valid_loss)
        improved, should_stop = early_stopping.step(valid_loss, model)

        if epoch % 10 == 0 or improved:
            mark = "*" if improved else ""
            print(f"  Epoch {epoch:3d}: Train={train_loss:.6f}, Valid={valid_loss:.6f} {mark}")

        if should_stop:
            print(f"  Early stopping at epoch {epoch}")
            break

    results_by_L[L] = {
        'valid_loss': early_stopping.best,
        'model_state': early_stopping.best_state,
        'train_losses': train_losses,
        'valid_losses': valid_losses
    }

    if early_stopping.best < best_valid_loss:
        best_valid_loss = early_stopping.best
        best_L = L
        best_model_state = early_stopping.best_state

    print(f"  Best valid loss for L={L}: {early_stopping.best:.6f}")

print(f"\n{'='*60}")
print(f"BEST MODEL: L={best_L} with valid_loss={best_valid_loss:.6f}")
print(f"{'='*60}")

"""##Cross attention"""

for L in L_VALUES:
    print(f"\n{'='*60}")
    print(f"TRAINING iTransformer WITH L = {L}")
    print(f"{'='*60}")

    train_dataset = StockMultiDayNewsDataset(
        train_df, train_df_original, feature_cols,
        text_embeddings_cache,
        horizons=H_LIST, window=L,
        embedding_dim=EMBEDDING_DIM,
        num_news_days=NEWS_LOOKBACK_DAYS
    )
    valid_dataset = StockMultiDayNewsDataset(
        valid_df, valid_df_original, feature_cols,
        text_embeddings_cache,
        horizons=H_LIST, window=L,
        embedding_dim=EMBEDDING_DIM,
        num_news_days=NEWS_LOOKBACK_DAYS
    )

    print(f"Train samples: {len(train_dataset)}, Valid samples: {len(valid_dataset)}")

    if len(train_dataset) < 10 or len(valid_dataset) < 5:
        print(f"Skipping L={L}: Not enough samples")
        continue

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    num_features = len(feature_cols)

    if USE_CROSS_ATTENTION:
        model = iTransformerCrossAttentionMultiDayNews(
            seq_len=L,
            num_variates=num_features,
            num_targets=len(H_LIST),
            d_model=D_MODEL,
            n_heads=N_HEADS,
            e_layers=E_LAYERS,
            d_ff=D_FF,
            dropout=DROPOUT,
            embedding_dim=EMBEDDING_DIM,
            num_news_days=NEWS_LOOKBACK_DAYS,
            num_attention_heads=4,
            use_text=USE_TEXT
        ).to(device)
    else:
        model = iTransformerConcatMultiDayNews(
            seq_len=L,
            num_variates=num_features,
            num_targets=len(H_LIST),
            d_model=D_MODEL,
            n_heads=N_HEADS,
            e_layers=E_LAYERS,
            d_ff=D_FF,
            dropout=DROPOUT,
            embedding_dim=EMBEDDING_DIM,
            num_news_days=NEWS_LOOKBACK_DAYS,
            text_proj_dim=64,
            use_text=USE_TEXT
        ).to(device)

    print(f"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    criterion = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3
    )
    early_stopping = EarlyStopping(patience=7, min_delta=1e-5)

    EPOCHS = 100
    train_losses, valid_losses = [], []

    for epoch in range(1, EPOCHS + 1):
        model.train()
        running_train = 0.0
        for X_batch, text_batch, y_batch, _, _ in train_loader:
            X_batch = X_batch.to(device)
            text_batch = text_batch.to(device)
            y_batch = y_batch.to(device)

            optimizer.zero_grad()

            if USE_TEXT:
                preds, _ = model(X_batch, text_batch)
            else:
                preds, _ = model(X_batch)

            loss = criterion(preds, y_batch)
            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_train += loss.item() * X_batch.size(0)

        train_loss = running_train / len(train_dataset)
        train_losses.append(train_loss)

        model.eval()
        running_valid = 0.0
        with torch.no_grad():
            for X_batch, text_batch, y_batch, _, _ in valid_loader:
                X_batch = X_batch.to(device)
                text_batch = text_batch.to(device)
                y_batch = y_batch.to(device)

                if USE_TEXT:
                    preds, _ = model(X_batch, text_batch)
                else:
                    preds, _ = model(X_batch)

                loss = criterion(preds, y_batch)
                running_valid += loss.item() * X_batch.size(0)

        valid_loss = running_valid / len(valid_dataset)
        valid_losses.append(valid_loss)

        scheduler.step(valid_loss)
        improved, should_stop = early_stopping.step(valid_loss, model)

        if epoch % 10 == 0 or improved:
            mark = "*" if improved else ""
            print(f"  Epoch {epoch:3d}: Train={train_loss:.6f}, Valid={valid_loss:.6f} {mark}")

        if should_stop:
            print(f"  Early stopping at epoch {epoch}")
            break

    results_by_L[L] = {
        'valid_loss': early_stopping.best,
        'model_state': early_stopping.best_state,
        'train_losses': train_losses,
        'valid_losses': valid_losses
    }

    if early_stopping.best < best_valid_loss:
        best_valid_loss = early_stopping.best
        best_L = L
        best_model_state = early_stopping.best_state

    print(f"  Best valid loss for L={L}: {early_stopping.best:.6f}")

print(f"\n{'='*60}")
print(f"BEST MODEL: L={best_L} with valid_loss={best_valid_loss:.6f}")
print(f"{'='*60}")

"""#Plot Training Results

##Fusion concatenate
"""

# Plot training results
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: So s√°nh valid loss c√°c L
L_list = list(results_by_L.keys())
valid_losses_list = [results_by_L[L]['valid_loss'] for L in L_list]

axes[0].bar(range(len(L_list)), valid_losses_list, color='steelblue')
axes[0].set_xticks(range(len(L_list)))
axes[0].set_xticklabels([f'L={L}' for L in L_list])
axes[0].set_ylabel('Best Valid Loss')
axes[0].set_title('Validation Loss by Window Size (L)')
axes[0].axhline(y=best_valid_loss, color='red', linestyle='--', label=f'Best: L={best_L}')
axes[0].legend()

# Plot 2: Training curves for best L
if best_L in results_by_L:
    axes[1].plot(results_by_L[best_L]['train_losses'], label='Train')
    axes[1].plot(results_by_L[best_L]['valid_losses'], label='Valid')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].set_title(f'iTransformer + AITeamVN/Vietnamese_Embedding Training - L={best_L}')
    axes[1].legend()
    axes[1].grid(True)

plt.tight_layout()
plt.show()

"""##Cross attention"""

# Plot training results
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot 1: So s√°nh valid loss c√°c L
L_list = list(results_by_L.keys())
valid_losses_list = [results_by_L[L]['valid_loss'] for L in L_list]

axes[0].bar(range(len(L_list)), valid_losses_list, color='steelblue')
axes[0].set_xticks(range(len(L_list)))
axes[0].set_xticklabels([f'L={L}' for L in L_list])
axes[0].set_ylabel('Best Valid Loss')
axes[0].set_title('Validation Loss by Window Size (L)')
axes[0].axhline(y=best_valid_loss, color='red', linestyle='--', label=f'Best: L={best_L}')
axes[0].legend()

# Plot 2: Training curves for best L
if best_L in results_by_L:
    axes[1].plot(results_by_L[best_L]['train_losses'], label='Train')
    axes[1].plot(results_by_L[best_L]['valid_losses'], label='Valid')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].set_title(f'iTransformer + AITeamVN/Vietnamese_Embedding Training - L={best_L}')
    axes[1].legend()
    axes[1].grid(True)

plt.tight_layout()
plt.show()

"""# Evaluation on Test Set"""

test_dataset = StockMultiDayNewsDataset(
    test_df, test_df_original, feature_cols,
    text_embeddings_cache,
    horizons=H_LIST, window=best_L,
    embedding_dim=EMBEDDING_DIM,
    num_news_days=NEWS_LOOKBACK_DAYS
)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"Test samples: {len(test_dataset)}")

num_features = len(feature_cols)

if USE_CROSS_ATTENTION:
    final_model = iTransformerCrossAttentionMultiDayNews(
        seq_len=best_L,
        num_variates=num_features,
        num_targets=len(H_LIST),
        d_model=D_MODEL,
        n_heads=N_HEADS,
        e_layers=E_LAYERS,
        d_ff=D_FF,
        dropout=DROPOUT,
        embedding_dim=EMBEDDING_DIM,
        num_news_days=NEWS_LOOKBACK_DAYS,
        num_attention_heads=4,
        use_text=USE_TEXT
    ).to(device)
else:
    final_model = iTransformerConcatMultiDayNews(
        seq_len=best_L,
        num_variates=num_features,
        num_targets=len(H_LIST),
        d_model=D_MODEL,
        n_heads=N_HEADS,
        e_layers=E_LAYERS,
        d_ff=D_FF,
        dropout=DROPOUT,
        embedding_dim=EMBEDDING_DIM,
        num_news_days=NEWS_LOOKBACK_DAYS,
        text_proj_dim=64,
        use_text=USE_TEXT
    ).to(device)

final_model.load_state_dict(best_model_state)
final_model.eval()
print("Best model loaded")

all_preds = []
all_targets = []
all_base_prices = []
all_actual_prices = []
all_attention_weights = []

with torch.no_grad():
    for X_batch, text_batch, y_batch, base_price_batch, actual_price_batch in test_loader:
        X_batch = X_batch.to(device)
        text_batch = text_batch.to(device)

        if USE_TEXT:
            preds, attn_weights = final_model(X_batch, text_batch)
        else:
            preds, attn_weights = final_model(X_batch)

        all_preds.append(preds.cpu().numpy())
        all_targets.append(y_batch.numpy())
        all_base_prices.append(base_price_batch.numpy())
        all_actual_prices.append(actual_price_batch.numpy())

        if attn_weights is not None:
            all_attention_weights.append(attn_weights.cpu().numpy())

all_preds = np.concatenate(all_preds, axis=0)
all_targets = np.concatenate(all_targets, axis=0)
all_base_prices = np.concatenate(all_base_prices, axis=0)
all_actual_prices = np.concatenate(all_actual_prices, axis=0)

print(f"Predictions shape: {all_preds.shape}")

"""##Fusion concatenate"""

print("\n=== TEST SET RESULTS (ACTUAL PRICE - VND) ===")

for i, h in enumerate(H_LIST):
    pred_returns  = all_preds[:, i]
    actual_returns = all_targets[:, i]

    # Chuy·ªÉn sang gi√°
    pred_prices   = all_base_prices * (1 + pred_returns)
    actual_prices = all_actual_prices[:, i]

    # MAE, RMSE, R2 tr√™n PRICE
    mae_price  = mean_absolute_error(actual_prices, pred_prices)
    rmse_price = root_mean_squared_error(actual_prices, pred_prices)
    r2_price   = r2_score(actual_prices, pred_prices)

    # Chu·∫©n ho√° theo gi√° trung b√¨nh (normalized error)
    mean_price = np.mean(actual_prices)
    mae_norm   = mae_price / mean_price
    rmse_norm  = rmse_price / mean_price

    # MAPE tr√™n PRICE
    mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100

    print(
        f"H= {h:>2}: "
        f"MAE={mae_price:,.2f} VND | "
        f"RMSE={rmse_price:,.2f} VND | "
        f"MAE_norm={mae_norm:.5f} | "
        f"RMSE_norm={rmse_norm:.5f} | "
        f"MAPE={mape:.2f}% | "
        f"R2={r2_price:.5f}"
    )

# T√≠nh pred_prices tr∆∞·ªõc khi plot
pred_prices = np.zeros_like(all_preds)  # [N, 5]
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_preds[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

# Ki·ªÉm tra shape
print(f"pred_prices shape: {pred_prices.shape}")
print(f"all_actual_prices shape: {all_actual_prices.shape}")

# Plot
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, h in enumerate(H_LIST):
    if i >= len(axes):
        break

    actual = all_actual_prices[:, i]
    pred = pred_prices[:, i]

    axes[i].scatter(actual, pred, alpha=0.5, s=20)

    # ƒê∆∞·ªùng perfect prediction
    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')

    axes[i].set_xlabel('Actual Price (VND)')
    axes[i].set_ylabel('Predicted Price (VND)')
    axes[i].set_title(f'Horizon H={h}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

# ·∫®n subplot th·ª´a
if len(H_LIST) < len(axes):
    axes[-1].axis('off')

plt.suptitle('Actual vs Predicted Prices', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Plot predictions vs actual cho m·ªôt v√†i horizons
fig, axes = plt.subplots(len(H_LIST), 1, figsize=(14, 3*len(H_LIST)))

for i, h in enumerate(H_LIST):
    ax = axes[i] if len(H_LIST) > 1 else axes

    pred_prices = all_base_prices * (1 + all_preds[:, i])
    actual_prices = all_actual_prices[:, i]

    ax.plot(actual_prices, label='Actual', alpha=0.7)
    ax.plot(pred_prices, label='Predicted', alpha=0.7)
    ax.set_title(f'Horizon H={h}')
    ax.set_xlabel('Sample')
    ax.set_ylabel('Price')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""##Cross attention"""

print("\n=== TEST SET RESULTS (ACTUAL PRICE - VND) ===")

for i, h in enumerate(H_LIST):
    pred_returns  = all_preds[:, i]
    actual_returns = all_targets[:, i]

    # Chuy·ªÉn sang gi√°
    pred_prices   = all_base_prices * (1 + pred_returns)
    actual_prices = all_actual_prices[:, i]

    # MAE, RMSE, R2 tr√™n PRICE
    mae_price  = mean_absolute_error(actual_prices, pred_prices)
    rmse_price = root_mean_squared_error(actual_prices, pred_prices)
    r2_price   = r2_score(actual_prices, pred_prices)

    # Chu·∫©n ho√° theo gi√° trung b√¨nh (normalized error)
    mean_price = np.mean(actual_prices)
    mae_norm   = mae_price / mean_price
    rmse_norm  = rmse_price / mean_price

    # MAPE tr√™n PRICE
    mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100

    print(
        f"H= {h:>2}: "
        f"MAE={mae_price:,.2f} VND | "
        f"RMSE={rmse_price:,.2f} VND | "
        f"MAE_norm={mae_norm:.5f} | "
        f"RMSE_norm={rmse_norm:.5f} | "
        f"MAPE={mape:.2f}% | "
        f"R2={r2_price:.5f}"
    )

# T√≠nh pred_prices tr∆∞·ªõc khi plot
pred_prices = np.zeros_like(all_preds)  # [N, 5]
for i in range(len(all_base_prices)):
    for j in range(len(H_LIST)):
        delta = all_base_prices[i] * all_preds[i, j]
        pred_prices[i, j] = all_base_prices[i] + delta

# Ki·ªÉm tra shape
print(f"pred_prices shape: {pred_prices.shape}")
print(f"all_actual_prices shape: {all_actual_prices.shape}")

# Plot
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

for i, h in enumerate(H_LIST):
    if i >= len(axes):
        break

    actual = all_actual_prices[:, i]
    pred = pred_prices[:, i]

    axes[i].scatter(actual, pred, alpha=0.5, s=20)

    # ƒê∆∞·ªùng perfect prediction
    min_val = min(actual.min(), pred.min())
    max_val = max(actual.max(), pred.max())
    axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')

    axes[i].set_xlabel('Actual Price (VND)')
    axes[i].set_ylabel('Predicted Price (VND)')
    axes[i].set_title(f'Horizon H={h}')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

# ·∫®n subplot th·ª´a
if len(H_LIST) < len(axes):
    axes[-1].axis('off')

plt.suptitle('Actual vs Predicted Prices', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Plot predictions vs actual cho m·ªôt v√†i horizons
fig, axes = plt.subplots(len(H_LIST), 1, figsize=(14, 3*len(H_LIST)))

for i, h in enumerate(H_LIST):
    ax = axes[i] if len(H_LIST) > 1 else axes

    pred_prices = all_base_prices * (1 + all_preds[:, i])
    actual_prices = all_actual_prices[:, i]

    ax.plot(actual_prices, label='Actual', alpha=0.7)
    ax.plot(pred_prices, label='Predicted', alpha=0.7)
    ax.set_title(f'Horizon H={h}')
    ax.set_xlabel('Sample')
    ax.set_ylabel('Price')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""# Sample Predictions

##Fusion concatenate
"""

print("\n" + "="*70)
print("SAMPLE PREDICTIONS (Last 5 samples)")
print("="*70)

for sample_idx in range(-5, 0):
    base_price = all_base_prices[sample_idx]
    print(f"\nüìä Sample {sample_idx}: Base Price = {base_price:,.0f} VND")
    print("-" * 60)

    for i, h in enumerate(H_LIST):
        pred_return = all_preds[sample_idx, i]
        actual_return = all_targets[sample_idx, i]
        actual_price = all_actual_prices[sample_idx, i]

        pred_delta = base_price * pred_return
        pred_price = base_price * (1 + pred_return)
        error = abs(actual_price - pred_price)
        error_pct = error / actual_price * 100

        direction = "üìà" if pred_return > 0 else "üìâ"

        print(f"  H={h:2d}: cumulative_return={pred_return:+.4f} ({pred_return*100:+.2f}%) {direction}")
        print(f"         Delta={pred_delta:+,.0f} VND")
        print(f"         Actual={actual_price:>10,.0f} VND | Pred={pred_price:>10,.0f} VND | Error={error:>7,.0f} VND ({error_pct:.2f}%)")

"""##Cross attention"""

print("\n" + "="*70)
print("SAMPLE PREDICTIONS (Last 5 samples)")
print("="*70)

for sample_idx in range(-5, 0):
    base_price = all_base_prices[sample_idx]
    print(f"\nüìä Sample {sample_idx}: Base Price = {base_price:,.0f} VND")
    print("-" * 60)

    for i, h in enumerate(H_LIST):
        pred_return = all_preds[sample_idx, i]
        actual_return = all_targets[sample_idx, i]
        actual_price = all_actual_prices[sample_idx, i]

        pred_delta = base_price * pred_return
        pred_price = base_price * (1 + pred_return)
        error = abs(actual_price - pred_price)
        error_pct = error / actual_price * 100

        direction = "üìà" if pred_return > 0 else "üìâ"

        print(f"  H={h:2d}: cumulative_return={pred_return:+.4f} ({pred_return*100:+.2f}%) {direction}")
        print(f"         Delta={pred_delta:+,.0f} VND")
        print(f"         Actual={actual_price:>10,.0f} VND | Pred={pred_price:>10,.0f} VND | Error={error:>7,.0f} VND ({error_pct:.2f}%)")

"""# Save Model"""

model_type = "crossattn" if USE_CROSS_ATTENTION else "concat"

save_dict = {
    'model_state_dict': best_model_state,
    'best_L': best_L,
    'scaler_robust': scaler_robust,
    'scaler_close': scaler_close,
    'features_robust': features_robust,
    'feature_cols': feature_cols,
    'horizons': H_LIST,
    'use_text': USE_TEXT,
    'use_cross_attention': USE_CROSS_ATTENTION,
    'embedding_model_name': MODEL_NAME,
    'embedding_dim': EMBEDDING_DIM,
    'num_news_days': NEWS_LOOKBACK_DAYS,
    'd_model': D_MODEL,
    'n_heads': N_HEADS,
    'e_layers': E_LAYERS,
    'd_ff': D_FF,
    'dropout': DROPOUT
}

model_short_name = MODEL_NAME.split("/")[-1].lower().replace("-", "_")
filename = f'itransformer_{model_short_name}_{model_type}_L{best_L}.pth'
torch.save(save_dict, filename)

print(f"\nModel saved as '{filename}'")

"""# Comparison: Cross-Attention and Concatenate Normally

To compare, you can:
1. Run the notebook with `USE_ATTENTION = True`
2. Run again with `USE_ATTENTION = False`
3. Compare the metrics

Also compare:
- `USE_TEXT = True` vs `USE_TEXT = False` to see the impact of news text
"""