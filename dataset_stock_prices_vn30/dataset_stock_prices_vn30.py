# -*- coding: utf-8 -*-
"""Dataset_Stock_Prices_VN30.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ln5pjslmhyhC6tiThsH-wo345HcwpgtA

#Import Libraries
"""

import pandas as pd
import numpy as np
import re
import os
from pandas_datareader import data as pdr

"""#Load Dataset and NaN, Dupplicate Processing"""

!apt-get install unrar

def parse_volume(x: str):
    # chuy·ªÉn "2.16M", "850K", "1.2B" -> s·ªë c·ªï phi·∫øu
    if pd.isna(x): return np.nan
    s = str(x).strip().replace(',', '')
    m = re.match(r'^([+-]?\d*\.?\d+)\s*([KMB]?)$', s, flags=re.I)
    if not m:
        # Tr∆∞·ªùng h·ª£p ch·ªâ l√† s·ªë: "1234567"
        try: return float(s)
        except: return np.nan
    val, suf = float(m.group(1)), m.group(2).upper()
    if suf == 'K': val *= 1_000
    elif suf == 'M': val *= 1_000_000
    elif suf == 'B': val *= 1_000_000_000
    return val

def clean_vn_price_csv(path):
    # ƒë·ªçc th·∫≥ng, ƒë·ªÉ pandas ƒëo√°n delimiter
    df = pd.read_csv(path)
    # chu·∫©n ho√° t√™n c·ªôt (strip, lower, b·ªè d·∫•u)
    colmap = {
        'Ng√†y':'date',
        'NgaÃÄy':'date',
        'L·∫ßn cu·ªëi':'close',
        'L√¢ÃÄn cu√¥ÃÅi':'close',
        'M·ªü':'open',
        'Cao':'high',
        'Th·∫•p':'low',
        'KL':'volume',
        'KLGD':'volume',
        '% Thay ƒë·ªïi':'pct_change',
        '% thay ƒë·ªïi':'pct_change',
        'Thay ƒë·ªïi %':'pct_change',
    }
    # map n·∫øu c√≥ c·ªôt t∆∞∆°ng ·ª©ng
    df = df.rename(columns={c: colmap.get(c, c) for c in df.columns})

    # Chu·∫©n ki·ªÉu ng√†y dd/mm/yyyy
    df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')

    # C√°c c·ªôt gi√°: b·ªè d·∫•u ph·∫©y h√†ng ngh√¨n r·ªìi float
    for c in ['open','high','low','close']:
        if c in df.columns:
            df[c] = (df[c].astype(str)
                           .str.replace(',', '', regex=False)
                           .str.replace(' ', '', regex=False))
            df[c] = pd.to_numeric(df[c], errors='coerce')

    # Kh·ªëi l∆∞·ª£ng
    if 'volume' in df.columns:
        df['volume'] = df['volume'].apply(parse_volume)

    # % thay ƒë·ªïi -> float (v√≠ d·ª• "0.51%")
    if 'pct_change' in df.columns:
        df['pct_change'] = (df['pct_change'].astype(str)
                                             .str.replace('%','', regex=False)
                                             .str.replace(',', '.', regex=False))
        df['pct_change'] = pd.to_numeric(df['pct_change'], errors='coerce') / 100.0

    # s·∫Øp x·∫øp c≈© -> m·ªõi v√† gi·ªØ c·ªôt chu·∫©n
    keep = [c for c in ['date','open','high','low','close','volume','pct_change'] if c in df.columns]
    df = df[keep].sort_values('date').dropna(subset=['date'])
    return df

!unrar x stock_prices.rar

# fpt = clean_vn_price_csv('/content/FPT.csv')
# fpt.head(), fpt.info()

folder = '/content/stock_prices'
frames = []

for fname in os.listdir(folder):
    if not fname.lower().endswith('.csv'):
        continue
    ticker = os.path.splitext(fname)[0].upper()
    df = clean_vn_price_csv(os.path.join(folder, fname))
    if len(df) == 0:
        print(f'‚ö†Ô∏è B·ªè qua {ticker}: r·ªóng sau khi l√†m s·∫°ch');
        continue
    df['ticker'] = ticker
    frames.append(df)

master = pd.concat(frames, ignore_index=True).sort_values(['date','ticker'])

# L·ªçc t·ª´ 2019-01-01
master = master[master['date'] >= pd.Timestamp('2018-12-31')]

# L∆∞u
# out_path = '/content/stock_prices_master.csv'
# master.to_csv(out_path, index=False)

# master = master.drop('pct_change', axis=1)

print(master.shape, master['ticker'].nunique())
master.head(50)

master['ticker'].unique()

def map_field(ticker):
    if ticker in ['ACB','BID','CTG','HDB','LPB','MBB',
                  'SHB','SSB','STB','TCB','TPB','VCB','VIB','VPB']:
        return 'Bank'
    elif ticker in ['BCM','GVR','VHM','VRE']:
        return 'Real Estate'
    elif ticker in ['MSN','VIC']:
        return 'Conglomerate'
    elif ticker in ['GAS','HPG','PLX','DGC']:
      return 'Energy and Raw Materials'
    elif ticker in ['MWG','SAB','VNM']:
      return 'Retail & Consumer Goods'
    else:
      return 'Other Technology and Services'

master['field'] = master['ticker'].apply(map_field)

master.head()

master.tail()

master.isna().sum().sort_values(ascending=False)

# L·ªçc ra c√°c h√†ng c√≥ √≠t nh·∫•t 1 NaN
rows_with_nan = master[master.isna().any(axis=1)]
len(rows_with_nan), rows_with_nan.head()

bad = master[master.isna().any(axis=1)]
print(bad)

so_dong_lap = master.duplicated().sum()
print(so_dong_lap)

master['date'] = pd.to_datetime(master['date'])

# L·ªçc BCM trong th√°ng 04/2019 (tr·ª´ ng√†y b·ªã thi·∫øu s·∫Ω t·ª± b·ªã NaN)
mask_month = (
    (master['ticker'] == 'BCM') &
    (master['date'] >= '2019-04-01') & (master['date'] <= '2019-04-30')
)

mean_apr = master.loc[mask_month, 'volume'].mean(skipna=True)  # trung b√¨nh 29 ng√†y c√≤n l·∫°i
master.loc[(master['ticker'] == 'BCM') & (master['date'] == '2019-04-17'), 'volume'] = mean_apr

# L·ªçc ra c√°c h√†ng c√≥ √≠t nh·∫•t 1 NaN
rows_with_nan = master[master.isna().any(axis=1)]
len(rows_with_nan), rows_with_nan.head()

"""#T√çNH TECHNICAL INDICATORS THEO T·ª™NG M√É"""

!pip install pandas_datareader pandas_ta

import pandas_ta as ta
master['date'] = pd.to_datetime(master['date'])
master = master.sort_values(['ticker','date'])
master.head()

def add_ta_features(df):
    df = df.sort_values('date').copy()
    # RSI(14)
    df['RSI_14']  = ta.rsi(df['close'], length=14)
    # MA(10), MA(50), MA(200)
    df['MA_10']   = ta.sma(df['close'], length=10)
    df['MA_50']   = ta.sma(df['close'], length=50)
    df['MA_200']  = ta.sma(df['close'], length=200)
    # MACD(12,26,9)
    macd = ta.macd(df['close'], fast=12, slow=26, signal=9)
    df['MACD_12']        = macd['MACD_12_26_9']
    df['MACD_SIGNAL_12_26'] = macd['MACDs_12_26_9']
    df['MACD_HIST_12_26_9']   = macd['MACDh_12_26_9']
    # ADX(14)
    adx = ta.adx(df['high'], df['low'], df['close'], length=14)
    df['ADX_14'] = adx['ADX_14']
    # STOCH(9,6) (m·∫∑c ƒë·ªãnh smooth_k=3)
    st = ta.stoch(df['high'], df['low'], df['close'], k=9, d=6, smooth_k=3)
    df['STOCH_K_9'] = st['STOCHk_9_6_3']
    df['STOCH_D_6'] = st['STOCHd_9_6_3']
    # STOCHRSI(14) ‚Äì l·∫•y %K
    srs = ta.stochrsi(df['close'], length=14)
    df['STOCHRSI_14'] = srs['STOCHRSIk_14_14_3_3']
    return df

master_ta = master.groupby('ticker', group_keys=False).apply(add_ta_features)

# C·∫Øt ‚Äúwarm-up‚Äù theo t·ª´ng m√£: b·∫Øt ƒë·∫ßu t·ª´ d√≤ng ƒë·∫ßu ti√™n m√† T·∫§T C·∫¢ TA != NaN
ta_cols = ['RSI_14','MA_10','MA_50','MA_200',
           'MACD_12','MACD_SIGNAL_12_26','MACD_HIST_12_26_9',
           'ADX_14','STOCH_K_9','STOCH_D_6','STOCHRSI_14']

def cut_from_first_full(df):
    df = df.sort_values('date').copy()
    mask_full = df[ta_cols].notna().all(axis=1)
    if not mask_full.any():      # m√£ qu√° ng·∫Øn (IPO mu·ªôn)
        return df.iloc[0:0]
    first_idx = mask_full.idxmax()
    start_date = df.loc[first_idx, 'date']
    return df[df['date'] >= start_date]

master_ta = master_ta.groupby('ticker', group_keys=False).apply(cut_from_first_full)

# L·ªçc ra c√°c h√†ng c√≥ √≠t nh·∫•t 1 NaN
rows_with_nan = master_ta[master_ta.isna().any(axis=1)]
len(rows_with_nan), rows_with_nan.head()

so_dong_lap = master_ta.duplicated().sum()
print(so_dong_lap)

master_ta.head()

master_ta.tail()

#L∆∞u
out_path = '/content/stock_prices_master_ta.csv'
master_ta.to_csv(out_path, index=False)

"""#GDP from Fred, USD_VND and CPI from Investing.com"""

!pip install -q fredapi

from fredapi import Fred
import pandas as pd
from pandas_datareader import data as pdr

dataset = pd.read_csv('/content/stock_prices_master_ta.csv')

dataset['date'] = pd.to_datetime(dataset['date'])
dataset = dataset.sort_values(['ticker','date'])

dataset.head()

dataset.describe()

dataset.shape, dataset.info()

"""##L·∫•y GDP t·ª´ FRED"""

fred = Fred(api_key='897a0069c65eaa41302baae2af5af953')
OBS_START = '2018-01-01'   # c·∫ßn l√πi 1 nƒÉm/th√°ng ƒë·ªÉ c√≥ gi√° tr·ªã cho 2020-01
OBS_END   = '2025-10-20'

# --- GDP cho Vi·ªát Nam ---
SERIES_GDP = 'MKTGDPVNA646NWDB'      # GDP current US$
gdp_ser = fred.get_series(SERIES_GDP, observation_start=OBS_START, observation_end=OBS_END)
gdp = gdp_ser.to_frame('GDP_val')
gdp.index = pd.to_datetime(gdp.index)

# L·∫•y 1 gi√° tr·ªã/nƒÉm (cu·ªëi nƒÉm), r·ªìi shift sang NƒÇM SAU
gdp_annual = gdp.resample('Y').last().copy()
gdp_annual['year_for_use'] = gdp_annual.index.year + 1   # nƒÉm m√† ta S·∫º s·ª≠ d·ª•ng gi√° tr·ªã c·ªßa nƒÉm tr∆∞·ªõc
gdp_map = dict(zip(gdp_annual['year_for_use'], gdp_annual['GDP_val']))
# => v√≠ d·ª• GDP_2019 ƒë∆∞·ª£c map cho year_for_use=2020

#√Åp c√°c mapping v√†o dataset (kh√¥ng h·ªÅ nh√¨n tr∆∞·ªõc)
dataset['YEAR'] = dataset['date'].dt.year
dataset['GDP √ó 10^9 (USD)'] = dataset['YEAR'].map(gdp_map)     # GDP c·ªßa NƒÇM TR∆Ø·ªöC
dataset['GDP √ó 10^9 (USD)'] = (dataset['GDP √ó 10^9 (USD)'] / 1e9).round(2)

#Gi·ªõi h·∫°n khung th·ªùi gian
dataset = dataset[(dataset['date'] >= '2020-01-01') & (dataset['date'] <= '2025-10-20')]

#D·ªçn c·ªôt t·∫°m & l∆∞u
dataset = dataset.drop(columns=['YEAR'])
# out_path = '/content/stock_prices_master_ta_with_GDP_CPI.csv'
# dataset.to_csv(out_path, index=False)
# print('Saved:', out_path)

dataset.head()

#M·ªôt d√≤ng/ nƒÉm (ƒë·∫ßu nƒÉm) ‚Äì to√†n b·ªô th·ªã tr∆∞·ªùng
check_year = (dataset.assign(year=dataset['date'].dt.year)
                      .query('year == year')
                      .sort_values('date')
                      .groupby('year', as_index=False)
                      .first()[['year','date','GDP √ó 10^9 (USD)']])
print(check_year)

"""##Web Scrapping for CPI from Investing.com"""

# C√ÄI M√îI TR∆Ø·ªúNG ·ªîN ƒê·ªäNH CHO COLAB
!apt-get update -y >/dev/null
!apt-get install -y wget >/dev/null

# C√†i Google Chrome stable (ƒë·ªìng b·ªô m·ªôt ngu·ªìn)
!wget -q -O /tmp/chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i /tmp/chrome.deb || apt-get -fy install -y  # t·ª± x·ª≠ l√Ω ph·ª• thu·ªôc

# Selenium + WebDriver Manager (t·ª± t·∫£i ƒë√∫ng chromedriver theo b·∫£n Chrome)
!pip -q install selenium==4.24.0 webdriver-manager==4.0.2 pandas==2.2.2

# Ki·ªÉm tra phi√™n b·∫£n
!google-chrome --version

import shutil, re, time
from datetime import datetime
import pandas as pd

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ƒê·∫∑t binary l√† Google Chrome stable v·ª´a c√†i
chrome_bin = shutil.which("google-chrome") or shutil.which("google-chrome-stable")

opts = Options()
if chrome_bin:
    opts.binary_location = chrome_bin
opts.add_argument("--headless=new")
opts.add_argument("--no-sandbox")
opts.add_argument("--disable-dev-shm-usage")
opts.add_argument("--window-size=1920,1080")
opts.add_argument("--lang=vi-VN")
opts.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36")
# (tu·ª≥ ch·ªçn) ƒë√¥i khi h·ªØu √≠ch tr√™n Colab
opts.add_argument("--disable-gpu")
opts.add_argument("--disable-software-rasterizer")

# WebDriverManager t·ª± ch·ªçn ƒë√∫ng chromedriver kh·ªõp phi√™n b·∫£n Chrome
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=opts)

url = "https://vn.investing.com/economic-calendar/vietnamese-cpi-1851"
driver.get(url)
wait = WebDriverWait(driver, 25)

# ƒê√≥ng popup cookie n·∫øu c√≥
for xp in [
    "//button[contains(.,'Ch·∫•p')]",
    "//button[contains(.,'ƒë·ªìng √Ω')]",
    "//button[contains(.,'Accept')]",
    "//button[contains(.,'I agree')]",
]:
    try:
        btn = WebDriverWait(driver, 3).until(EC.element_to_be_clickable((By.XPATH, xp)))
        btn.click(); break
    except Exception:
        pass

# M·ªü tab L·ªãch s·ª≠ (n·∫øu c·∫ßn)
try:
    hist_anchor = driver.find_element(By.CSS_SELECTOR, 'a[name="history"]')
    driver.execute_script("arguments[0].click();", hist_anchor)
except Exception:
    pass

wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "#eventHistoryTable1851 tbody")))
print("> B·∫£ng l·ªãch s·ª≠ ƒë√£ s·∫µn s√†ng.")

# =======================
# 3) Click 'Hi·ªÉn th·ªã th√™m' t·ªõi m·ªëc 27/12/2019
# =======================
START_DATE = datetime.strptime("27/12/2019", "%d/%m/%Y")
END_DATE   = datetime.strptime("06/10/2025", "%d/%m/%Y")

def last_loaded_date():
    rows = driver.find_elements(By.CSS_SELECTOR, "#eventHistoryTable1851 tbody tr")
    if not rows:
        return None
    # h√†ng cu·ªëi l√† c≈© nh·∫•t trong b·∫£ng
    last_td0 = rows[-1].find_elements(By.CSS_SELECTOR, "td")[0].text
    m = re.search(r"(\d{2}/\d{2}/\d{4})", last_td0)
    if not m:
        return None
    return datetime.strptime(m.group(1), "%d/%m/%Y")

clicks = 0
prev_rows = 0

while True:
    mind = last_loaded_date()
    rows_now = len(driver.find_elements(By.CSS_SELECTOR, "#eventHistoryTable1851 tbody tr"))
    if mind and mind <= START_DATE:
        break
    if rows_now == prev_rows and clicks > 0:
        # kh√¥ng load th√™m n·ªØa
        break
    prev_rows = rows_now

    try:
        btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "#showMoreHistory1851 a")))
        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", btn)
        driver.execute_script("arguments[0].click();", btn)
        clicks += 1
        # ch·ªù s·ªë h√†ng tƒÉng ho·∫∑c ng√†y c≈© h∆°n xu·∫•t hi·ªán
        WebDriverWait(driver, 20).until(
            lambda d: len(d.find_elements(By.CSS_SELECTOR, "#eventHistoryTable1851 tbody tr")) > rows_now
        )
        time.sleep(0.7)
    except Exception:
        # h·∫øt n√∫t ho·∫∑c l·ªói click -> d·ª´ng
        break

print(f"> ƒê√£ b·∫•m 'Hi·ªÉn th·ªã th√™m' {clicks} l·∫ßn. Ng√†y c≈© nh·∫•t hi·ªán c√≥: {last_loaded_date().strftime('%d/%m/%Y') if last_loaded_date() else 'N/A'}")

# =======================
# 4) Tr√≠ch 2 c·ªôt: Ng√†y Ph√°t H√†nh & Th·ª±c t·∫ø
# =======================
rows = driver.find_elements(By.CSS_SELECTOR, "#eventHistoryTable1851 tbody tr")
data = []

for r in rows:
    tds = r.find_elements(By.CSS_SELECTOR, "td")
    if len(tds) < 3:
        continue

    # C·ªôt 1: Ng√†y Ph√°t H√†nh (vd: "06/10/2025 (Th√°ng 9)")
    date_raw = tds[0].text.strip()
    m = re.search(r"(\d{2}/\d{2}/\d{4})", date_raw)
    if not m:
        continue
    d = datetime.strptime(m.group(1), "%d/%m/%Y")

    # L·ªçc theo kho·∫£ng y√™u c·∫ßu
    if not (START_DATE <= d <= END_DATE):
        continue

    # C·ªôt 3: Th·ª±c t·∫ø (c√≥ th·ªÉ r·ªóng ho·∫∑c ‚Äú‚Äî‚Äù)
    actual_raw = tds[2].text.replace("\xa0", " ").strip()
    # gi·ªØ nguy√™n d·∫°ng "3.38%" (kh√¥ng √©p float). N·∫øu mu·ªën float %, b·ªè comment 2 d√≤ng d∆∞·ªõi:
    # m2 = re.search(r"(-?\d+(?:[.,]\d+)?)\s*%", actual_raw)
    # actual_val = float(m2.group(1).replace(",", ".")) if m2 else None

    data.append({
        "NgayPhatHanh": d.strftime("%Y-%m-%d"),
        "ThucTe": actual_raw or None
    })

driver.quit()

df = pd.DataFrame(data).sort_values("NgayPhatHanh")
print(df.head(), "\n---\n", df.tail(), f"\n\nT·ªïng s·ªë d√≤ng: {len(df)}")

out_path = "/content/vn_cpi.csv"
df.to_csv(out_path, index=False, encoding="utf-8")
print(f"\n‚úÖ ƒê√£ l∆∞u: {out_path}")

"""Sau khi t·∫£i vn_cpi_investing_2019-12-27_to_2025-10-06.csv v·ªÅ th√¨ nh√≥m c√≥ ch·ªânh l·∫°i th·ªß c√¥ng d·ªØ li·ªáu c·ªßa c·ªôt NgayPhatHanh t·ª´ ng√†y 29/07/2024

**Gi·∫£i th√≠ch**:
- B·∫£ng ‚ÄúL·ªãch s·ª≠‚Äù c·ªßa Investing hi·ªÉn th·ªã theo ng√†y b√†i vi·∫øt ƒë∆∞·ª£c ƒëƒÉng tr√™n site, ƒë√¥i khi l·ªách sang ƒë·∫ßu th√°ng k·∫ø ti·∫øp, nh∆∞ng trong ngo·∫∑c v·∫´n ghi ‚Äú(Th√°ng X)‚Äù ‚Äì t·ª©c k·ª≥ tham chi·∫øu c·ªßa ch·ªâ s·ªë CPI.

- N·∫øu gi·ªØ nguy√™n ng√†y ƒëƒÉng b√†i (v√≠ d·ª• 06/09/2024 cho Th√°ng 8), khi gh√©p d·ªØ li·ªáu t√†i ch√≠nh theo th·ªùi gian s·∫Ω l√†m sai l·ªách th√°ng (CPI th√°ng 8 b·ªã r∆°i v√†o th√°ng 9) ‚Üí g√¢y misalignment v√† ti·ªÅm ·∫©n look-ahead bias khi hu·∫•n luy·ªán m√¥ h√¨nh.

**Quy ∆∞·ªõc ƒë·ªÉ ch·ªânh s·ª≠a**:

- L·∫•y ng√†y 29 c·ªßa h√†ng th√°ng, ri√™ng th√°ng 2 l√† ng√†y 28

**V√≠ d·ª•**:
- 29/07/2024 (Th√°ng 7) -> 29/07/2024
- 06/09/2024 (Th√°ng 8) -> 29/08/2024
- ....
- 06/10/2025 (Th√°ng 9) -> 29/09/2025

###read cpi csv from web and processing
"""

import pandas as pd
cpi_reader = pd.read_csv('vn_cpi.csv', sep=';', dtype=str, encoding='utf-8')
cpi_reader.head()

# Parse ng√†y dd/mm/yyyy  -> datetime
cpi_reader['NgayPhatHanh'] = pd.to_datetime(
    cpi_reader['NgayPhatHanh'].astype(str).str.strip(),
    format='%d/%m/%Y', errors='coerce'
)

# Chuy·ªÉn ƒë·ªïi sang d·∫°ng Period (Th√°ng)
cpi_reader['NgayPhatHanh'] = cpi_reader['NgayPhatHanh'].dt.to_period('M')

cpi_reader.head()

cpi_reader['ThucTe'] = (
    cpi_reader['ThucTe'].astype(str)
      .str.replace('\u00a0',' ', regex=False)    # b·ªè NBSP
      .str.extract(r'(-?\d+(?:[.,]\d+)?)')[0]    # l·∫•y ph·∫ßn s·ªë
      .str.replace(',','.')                      # ph·∫©y -> ch·∫•m
      .astype(float) / 100
)

print(cpi_reader[['NgayPhatHanh','ThucTe']])

#B·∫£o ƒë·∫£m ki·ªÉu ng√†y
dataset['date'] = pd.to_datetime(dataset['date'])

# N·∫øu c·ªôt NgayPhatHanh ƒëang l√† Period[M], ƒë·ªïi v·ªÅ Timestamp (ƒë·∫ßu th√°ng)
if pd.api.types.is_period_dtype(cpi_reader['NgayPhatHanh']):
    cpi_reader['NgayPhatHanh'] = cpi_reader['NgayPhatHanh'].dt.to_timestamp(how='start')

# 1) T·∫°o kh√≥a th√°ng cho dataset
dataset['month_key'] = dataset['date'].dt.to_period('M')

# 2) Mapping CPI theo lu·∫≠t: CPI th√°ng M ‚Üí g√°n cho c√°c m·∫´u thu·ªôc TH√ÅNG M+1
cpi_map = cpi_reader[['NgayPhatHanh', 'ThucTe']].copy()     # ThucTe ƒë√£ l√† ratio 0..1
cpi_map['month_key'] = (cpi_map['NgayPhatHanh'] + pd.offsets.MonthBegin(1)).dt.to_period('M')
cpi_map = (cpi_map
           .drop_duplicates('month_key', keep='last')       # ph√≤ng tr√πng th√°ng
           [['month_key','ThucTe']].rename(columns={'ThucTe':'CPI'}))

# 3) Merge v√†o dataset
dataset = dataset.merge(cpi_map, on='month_key', how='left').drop(columns='month_key')

# Ki·ªÉm tra: th√°ng 2025-10 ph·∫£i nh·∫≠n CPI c·ªßa 2025-09
print(dataset.loc[dataset['date'].dt.to_period('M')=='2025-10', ['date','CPI']])

dataset.head()

dataset.info()

dataset.shape

"""##USD_VND"""

usd_vnd = pd.read_csv(
    'USD_VND.csv',
    sep=';',                 # file d√πng d·∫•u ';'
    decimal='.',             # d·∫•u th·∫≠p ph√¢n l√† '.'
    thousands=',',           # d·∫•u ph√¢n t√°ch h√†ng ngh√¨n l√† ','
    parse_dates=['date'],    # ƒë·ªïi c·ªôt date sang datetime
    dayfirst=True,           # ƒë·ªãnh d·∫°ng dd/mm/yyyy trong file
    dtype={'USD_VND':'float64'}
)

print(usd_vnd.dtypes)

usd_vnd

# G√°n c·ªôt usd_vnd v√†o dataset theo ng√†y
dataset = dataset.merge(
    usd_vnd.rename(columns={'USD_VND': 'usd_vnd'})[['date', 'usd_vnd']],
    on='date',
    how='left'
)

# Ki·ªÉm tra nhanh
print(dataset[['date', 'usd_vnd']].head())
print('S·ªë ng√†y kh√¥ng kh·ªõp t·ª∑ gi√°:', dataset['usd_vnd'].isna().sum())

dataset.head()

"""#Check dataset"""

# Ki·ªÉm tra xem c√≥ d√≤ng n√†o tr√πng ho√†n to√†n kh√¥ng
duplicates = dataset[dataset.duplicated()]
print("S·ªë d√≤ng b·ªã l·∫∑p:", len(duplicates))

# N·∫øu b·∫°n mu·ªën xem c·ª• th·ªÉ c√°c d√≤ng l·∫∑p:
print(duplicates.head())

# Ho·∫∑c ƒë·∫øm theo kh√≥a c·ª• th·ªÉ (v√≠ d·ª•: theo 'date' + 'ticker')
dup_by_key = dataset[dataset.duplicated(subset=['date', 'ticker'], keep=False)]
print(dup_by_key.sort_values(['date','ticker']))

# ƒê·∫øm t·ªïng s·ªë gi√° tr·ªã NaN trong to√†n DataFrame
print("T·ªïng s·ªë √¥ NaN:", dataset.isna().sum().sum())

# ƒê·∫øm NaN theo t·ª´ng c·ªôt
print("S·ªë NaN theo c·ªôt:\n", dataset.isna().sum())

# Xem c√°c d√≤ng c√≥ √≠t nh·∫•t 1 gi√° tr·ªã NaN
nan_rows = dataset[dataset.isna().any(axis=1)]
print("S·ªë d√≤ng c√≥ NaN:", len(nan_rows))

dataset.info()

dataset.shape

out_path = '/content/dataset_stock_prices_VN30.csv'
dataset.to_csv(out_path, index=False)
print('Saved:', out_path)

"""#Correlation Matrix and Features Selection"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
dataset = pd.read_csv('/content/dataset_stock_prices_VN30.csv')
print(dataset.head())
print(dataset.columns)

# L·∫•y danh s√°ch c√°c features s·ªë (b·ªè qua date, ticker, field)
numeric_cols = dataset.select_dtypes(include=[np.number]).columns.tolist()
feature_cols = [col for col in numeric_cols if col != 'close']

# Dictionary l∆∞u correlation c·ªßa m·ªói m√£
correlations_by_ticker = {}

# T√≠nh correlation v·ªõi close cho T·ª™NG M√É
for ticker in dataset['ticker'].unique():
    ticker_data = dataset[dataset['ticker'] == ticker]

    # T√≠nh correlation c·ªßa c√°c features v·ªõi close
    corr_with_close = ticker_data[numeric_cols].corr()['close']
    correlations_by_ticker[ticker] = corr_with_close.drop('close')

import matplotlib.pyplot as plt
import seaborn as sns

#Heatmap t·ªïng h·ª£p - T·∫§T C·∫¢ M√É C√ôNG L√öC
plt.figure(figsize=(20, 12))

# Chuy·ªÉn dictionary th√†nh DataFrame ƒë·ªÉ v·∫Ω
correlation_df = pd.DataFrame(correlations_by_ticker).T  # T ƒë·ªÉ transpose
correlation_df = correlation_df.sort_index()  # S·∫Øp x·∫øp theo t√™n m√£

# V·∫Ω heatmap
sns.heatmap(correlation_df,
            annot=True,           # Hi·ªÉn th·ªã gi√° tr·ªã
            fmt='.2f',            # Format 2 s·ªë th·∫≠p ph√¢n
            cmap='RdBu_r',        # Red-Blue reversed
            center=0,             # M√†u tr·∫Øng ·ªü gi·ªØa (0)
            vmin=-1, vmax=1,      # Range t·ª´ -1 ƒë·∫øn 1
            cbar_kws={'label': 'Correlation with Close'},
            linewidths=0.5)

plt.title('Ma Tr·∫≠n T∆∞∆°ng Quan v·ªõi Close - 29 M√£ VN30', fontsize=16, fontweight='bold')
plt.xlabel('Features', fontsize=12)
plt.ylabel('M√£ c·ªï phi·∫øu', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""##For all tickers"""

# #T·∫†O correlation_df T·ª™ correlations_by_ticker
# correlation_df = pd.DataFrame(correlations_by_ticker).T

# # T√¨m features c√≥ √çT NH·∫§T 5 m√£ c√≥ |correlation| >= 0.3
# threshold = 0.3
# min_stocks = 5
# features_to_remove = []
# features_to_keep = []

# print("="*70)
# print(f"DANH S√ÅCH FEATURES KH√îNG ƒê·∫†T Y√äU C·∫¶U (< {min_stocks} m√£ c√≥ |corr| >= {threshold}):")
# print("="*70)

# for feature in feature_cols:
#     count_high_corr = 0
#     max_corr = 0

#     for ticker in correlations_by_ticker:
#         corr_value = abs(correlations_by_ticker[ticker][feature])
#         max_corr = max(max_corr, corr_value)
#         if corr_value >= threshold:
#             count_high_corr += 1

#     if count_high_corr < min_stocks:
#         features_to_remove.append(feature)
#         print(f"'{feature}': ch·ªâ {count_high_corr}/{len(correlations_by_ticker)} m√£ c√≥ |corr| >= {threshold} (Max = {max_corr:.3f})")
#     else:
#         features_to_keep.append(feature)

# print(f"\nüìä T√ìM T·∫ÆT:")
# print(f"- T·ªïng features ban ƒë·∫ßu: {len(feature_cols)}")
# print(f"- Features b·ªã lo·∫°i: {len(features_to_remove)}")
# print(f"- Features gi·ªØ l·∫°i: {len(features_to_keep)}")

# print(f"\n‚úÖ FEATURES GI·ªÆ L·∫†I ({len(features_to_keep)}):")
# for feat in features_to_keep:
#     count = sum(1 for ticker in correlations_by_ticker
#                 if abs(correlations_by_ticker[ticker][feat]) >= threshold)
#     print(f"  {feat:30s}: {count}/{len(correlations_by_ticker)} m√£ c√≥ |corr| >= {threshold}")

# # V·∫Ω heatmap CH·ªà V·ªöI FEATURES GI·ªÆ L·∫†I
# plt.figure(figsize=(16, 12))

# correlation_df_filtered = correlation_df[features_to_keep]

# sns.heatmap(correlation_df_filtered,
#             annot=True,
#             fmt='.2f',
#             cmap='RdBu_r',
#             center=0,
#             vmin=-1, vmax=1,
#             cbar_kws={'label': 'Correlation with Close'},
#             linewidths=0.5)

# plt.title(f'Ma Tr·∫≠n T∆∞∆°ng Quan SAU KHI L·ªåC - Features c√≥ >= {min_stocks} m√£ v·ªõi |corr| >= {threshold}',
#           fontsize=14, fontweight='bold')
# plt.xlabel('Features', fontsize=12)
# plt.ylabel('M√£ c·ªï phi·∫øu', fontsize=12)
# plt.xticks(rotation=45, ha='right')
# plt.tight_layout()
# plt.show()

# # C·∫≠p nh·∫≠t dataframe - gi·ªØ l·∫°i features ƒë∆∞·ª£c ch·ªçn + c√°c c·ªôt c·∫ßn thi·∫øt
# columns_to_keep = ['date', 'ticker', 'field', 'close'] + features_to_keep

# # T·∫°o dataframe m·ªõi v·ªõi c√°c c·ªôt ƒë∆∞·ª£c ch·ªçn
# dataset_filtered = dataset[columns_to_keep].copy()

# # Ki·ªÉm tra k·∫øt qu·∫£
# print("="*70)
# print("TH√îNG TIN DATASET SAU KHI L·ªåC:")
# print("="*70)
# print(f"S·ªë d√≤ng: {len(dataset_filtered)}")
# print(f"S·ªë c·ªôt: {len(dataset_filtered.columns)}")
# print(f"\nDanh s√°ch c·ªôt sau khi l·ªçc:")
# print(dataset_filtered.columns.tolist())

# # Xem m·∫´u d·ªØ li·ªáu
# print("\nM·∫´u d·ªØ li·ªáu sau khi l·ªçc:")
# print(dataset_filtered.head())
# print(f"   - T·ªïng s·ªë features (kh√¥ng k·ªÉ date, ticker, field, close): {len(features_to_keep)}")
# print(f"   - T·ªïng s·ªë c·ªôt: {len(dataset_filtered.columns)}")
# print(f"   - T·ªïng s·ªë d√≤ng: {len(dataset_filtered)}")

# # Th·ªëng k√™ chi ti·∫øt
# print("\nüìä TH·ªêNG K√ä CHI TI·∫æT:")
# print(f"Dataset g·ªëc: {len(dataset.columns)} c·ªôt x {len(dataset)} d√≤ng")
# print(f"Dataset m·ªõi: {len(dataset_filtered.columns)} c·ªôt x {len(dataset_filtered)} d√≤ng")
# print(f"ƒê√£ lo·∫°i b·ªè: {len(dataset.columns) - len(dataset_filtered.columns)} c·ªôt kh√¥ng ƒë·∫°t y√™u c·∫ßu")

"""#Only Keeping √Å Ch√¢u (ACB)"""

# Copy dataset g·ªëc
acb_df = dataset.copy()

# Ch·ªâ gi·ªØ l·∫°i m√£ c·ªï phi·∫øu ACB
acb_df = acb_df[acb_df['ticker'] == 'ACB']

print(acb_df.shape)
acb_df.head()

"""##Keep All Features except pct_change"""

acb_df = acb_df.drop(['pct_change'], axis = 1)
acb_df.head()

acb_df.rename(columns={
    'MACD_12': 'MACD',
    'MACD_SIGNAL_12_26': 'MACD_SIGNAL',
    'MACD_HIST_12_26_9': 'MACD_HIST',
    'STOCH_K_9': 'STOCH_K',
    'STOCH_D_6': 'STOCH_D'
}, inplace=True)

print("Columns renamed successfully.")

acb_df.head()

out_path_acb = '/content/dataset_ACB_fullFeatures.csv'
acb_df.to_csv(out_path_acb, index=False)
print('Saved:', out_path_acb)

"""##Features Selection"""

# 2) Ch·ªâ l·∫•y c·ªôt s·ªë ƒë·ªÉ t√≠nh correlation
numeric_cols = acb_df.select_dtypes(include=[np.number]).columns.tolist()

# ƒê·∫£m b·∫£o c√≥ close trong numeric
if 'close' not in numeric_cols:
    raise ValueError("C·ªôt 'close' kh√¥ng ph·∫£i numeric ho·∫∑c kh√¥ng t·ªìn t·∫°i trong d·ªØ li·ªáu!")

# 3) Correlation c·ªßa numeric features v·ªõi close
corr_with_close = acb_df[numeric_cols].corr()['close'].drop('close')

# 4) L·ªçc feature theo ng∆∞·ª°ng (v√≠ d·ª• |corr| >= 0.299 ~ 0.3)
threshold = 0.299
selected_features = corr_with_close[abs(corr_with_close) >= threshold].index.tolist()

print(f"Selected numeric features (|corr| >= {threshold}):")
print(selected_features)

# 5) DataFrame cu·ªëi: gi·ªØ date/ticker/field + close + c√°c feature ƒë∆∞·ª£c ch·ªçn
meta_cols = [c for c in ['date', 'ticker', 'field'] if c in acb_df.columns]
keep_cols = meta_cols + ['close'] + selected_features

acb_selected_df = acb_df[keep_cols].copy()

print(acb_selected_df.shape)
display(acb_selected_df.head())

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Ch·ªâ l·∫•y c·ªôt s·ªë ƒë·ªÉ t√≠nh corr (b·ªè date/ticker/field t·ª± ƒë·ªông)
corr_df = acb_selected_df.select_dtypes(include=[np.number]).corr()

plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_df,
    annot=True,
    fmt='.2f',
    cmap='RdBu_r',
    center=0,
    vmin=-1, vmax=1,
    linewidths=0.5,
    cbar_kws={'label': 'Correlation'}
)

plt.title('Ma Tr·∫≠n T∆∞∆°ng Quan ‚Äì ACB (Features |corr| ‚â• 0.3)', fontsize=14, fontweight='bold')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

out_path_acb = '/content/dataset_ACB.csv'
acb_selected_df.to_csv(out_path_acb, index=False)
print('Saved:', out_path_acb)

print(acb_selected_df.shape)
acb_selected_df.head()

acb_selected_df.info()

acb_selected_df.describe()