# -*- coding: utf-8 -*-
"""Dataset_kaggle_news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UON8eiiGbt7xjitDs19zVPne17XWf4YP

#Download and import Libraries
"""

!pip install requests tqdm

import pandas as pd
import re
import numpy as np

import requests
import json
import csv
import time
import re
from tqdm import tqdm

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
from nltk.tokenize import word_tokenize

nltk.download('all')

!pip install underthesea

"""#Load Dataset and Fillter"""

# df = pd.read_excel("text_stock.xlsx")
# df.to_csv("text_stock.csv", index=False, encoding="utf-8-sig")

df = pd.read_csv("text_stock.csv")
df.head()

df.tail()

df.shape

df["time"] = pd.to_datetime(df["time"])

# --- TIME FILTER ---
df = df[df["time"] >= "2019-11-30"]

# --- CATEGORY FILTER ---
allowed_cats = ["Ch·ª©ng kho√°n", "T√†i ch√≠nh - Ch·ª©ng kho√°n"]
df = df[df["category"].isin(allowed_cats)]

# --- TAG KEYWORD FILTER ---
keywords = [
    "ACB", "√Å Ch√¢u",
    "v√†ng", "GDP", "Kinh t·∫ø",
    "t√†i ch√≠nh", "CPI",
    "t·ª∑ gi√°", "ƒë·ªëi ho√†i",
    "usd", "vnd","th·ªã tr∆∞·ªùng","ch·ª©ng kho√°n"
]

df["tags_clean"] = df["tags"].fillna("").str.lower()
keywords_lower = [k.lower() for k in keywords]

mask_tags = df["tags_clean"].apply(lambda x: any(k in x for k in keywords_lower))

filtered_df = df[mask_tags].copy()

filtered_df = filtered_df.sort_values(by="time", ascending=True)
filtered_df.head()

filtered_df.shape

# T·∫°o c·ªôt date
filtered_df["date"] = filtered_df["time"].dt.date

filtered_df.head()

# 1. X√≥a c·ªôt time n·∫øu c√≤n
if "time" in filtered_df.columns:
    filtered_df = filtered_df.drop(columns=["time"])

# 2. ƒê∆∞a c·ªôt date l√™n ƒë·∫ßu
cols = ["date"] + [col for col in filtered_df.columns if col != "date"]
filtered_df = filtered_df[cols]

# 3. Set index = date
filtered_df = filtered_df.set_index("date")

# 4. ƒê·∫£m b·∫£o index d·∫°ng datetime (r·∫•t quan tr·ªçng cho time-series)
filtered_df.index = pd.to_datetime(filtered_df.index)

filtered_df.head()

filtered_df = filtered_df.drop(columns=["tags_clean"])
filtered_df = filtered_df.drop(columns=["content_token_counts"])
filtered_df = filtered_df.drop(columns=["tags"])

filtered_df.shape

filtered_df.head()

filtered_df.tail()

"""##Th√™m Data cho ƒë·∫øn 20/10/2025"""

# ƒê·ªçc file CSV m·ªõi
df_new = pd.read_csv("vietnam_finance_news_dataset.csv")
df_new.head()

# Chu·∫©n h√≥a c·ªôt ng√†y
df_new["date"] = pd.to_datetime(df_new["date"])

# Reset index c·ªßa filtered_df n·∫øu ƒëang d√πng date l√†m index
filtered_df_reset = filtered_df.reset_index()

# ƒê·ªìng b·ªô c√°c c·ªôt gi·ªØa 2 dataframe
for col in filtered_df_reset.columns:
    if col not in df_new.columns:
        df_new[col] = None

# Gh√©p 2 dataframe
combined_df = pd.concat([filtered_df_reset, df_new], ignore_index=True)

# Sort theo ng√†y
combined_df = combined_df.sort_values("date")

# ƒê·∫∑t l·∫°i index
combined_df = combined_df.set_index("date")
combined_df.index = pd.to_datetime(combined_df.index)

combined_df.tail()

combined_df.head()

combined_df.shape

combined_df.to_csv('kaggle_news.csv', index=True, encoding='utf-8-sig')

"""#Preprocessing

- Lo·∫°i b·ªè k√Ω t·ª± l·ªói: \n, \t
- Lo·∫°i b·ªè emoji (n·∫øu c√≥)
- Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
- Thay k√≠ t·ª± HTML (&#39;, &quot;, &amp;, ‚Ä¶)
- Gi·ªØ nguy√™n k√Ω t·ª± quan tr·ªçng c·ªßa kinh t·∫ø: % . , / -
- Kh√¥ng x√≥a d·∫•u ti·∫øng Vi·ªát
- Kh√¥ng x√≥a s·ªë
"""

combined_df = pd.read_csv("kaggle_news.csv")
combined_df.head()

combined_df.tail()

import re
import html
import pandas as pd

def minimal_clean(text):
    """Preprocessing t·ªëi thi·ªÉu cho model embedding hi·ªán ƒë·∫°i"""
    if pd.isna(text):
        return ""

    text = str(text)

    # 1. Gi·∫£i m√£ HTML entities
    text = html.unescape(text)

    # 2. B·ªè HTML tags
    text = re.sub(r'<[^>]+>', ' ', text)

    # 3. B·ªè URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # 4. B·ªè emoji v√† k√Ω t·ª± l·∫° (gi·ªØ l·∫°i ch·ªØ, s·ªë, d·∫•u c√¢u c∆° b·∫£n, ti·∫øng Vi·ªát)
    text = re.sub(
        r"[^0-9a-zA-Z√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ"
        r"√ç√å·ªàƒ®·ªä√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê"
        r"√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá"
        r"√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë"
        r"%\.\,\/\-\+\$‚Ç´\(\)\:\!\?\s]",
        " ",
        text
    )

    # 5. Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = ' '.join(text.split())

    return text

#ch·ªâ cho content
combined_df["content_clean"] = combined_df["content"].astype(str).apply(minimal_clean)

combined_df[["content", "content_clean"]].head(20)

combined_df[["content", "content_clean"]].tail(20)

combined_df.to_csv('kaggle_news_preprocess.csv', index=True, encoding='utf-8-sig')

combined_df.head()

!pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Gi·∫£ s·ª≠ c·ªôt ƒë√£ l√† content_clean
text = " ".join(combined_df["content_clean"].astype(str).tolist())
len(text)

wc = WordCloud(
    width=1200,
    height=600,
    background_color="white"   # m√†u n·ªÅn, c√≤n m√†u ch·ªØ ƒë·ªÉ m·∫∑c ƒë·ªãnh
).generate(text)

plt.figure(figsize=(14, 7))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

combined_df["date"] = pd.to_datetime(combined_df["date"])
combined_df = combined_df.set_index("date")

mask_2024 = combined_df.index >= "2024-01-01"
text_2024 = " ".join(combined_df.loc[mask_2024, "content_clean"].astype(str))

wc_2024 = WordCloud(
    width=1200,
    height=600,
    background_color="white"
).generate(text_2024)

plt.figure(figsize=(14, 7))
plt.imshow(wc_2024, interpolation="bilinear")
plt.axis("off")
plt.show()

"""#ƒê·∫æM TOKENS C·ª¶A C·ªòT CONTENT"""

from transformers import AutoTokenizer
import pandas as pd

# Load tokenizers
print("Loading tokenizers...")
tok_phobert = AutoTokenizer.from_pretrained("vinai/phobert-base")
tok_vn_embed = AutoTokenizer.from_pretrained("AITeamVN/Vietnamese_Embedding")
tok_vn_doc = AutoTokenizer.from_pretrained("dangvantuan/vietnamese-document-embedding", trust_remote_code=True)

# ƒê·∫øm tokens
combined_df['tok_phobert'] = combined_df['content'].apply(lambda x: len(tok_phobert.encode(str(x))) if pd.notna(x) else 0)
combined_df['tok_vn_embed'] = combined_df['content'].apply(lambda x: len(tok_vn_embed.encode(str(x))) if pd.notna(x) else 0)
combined_df['tok_vn_doc'] = combined_df['content'].apply(lambda x: len(tok_vn_doc.encode(str(x))) if pd.notna(x) else 0)

# Th·ªëng k√™
print("\n" + "="*60)
print("MODEL                          | MAX  | % FIT  | MAX IN DATA")
print("="*60)
print(f"PhoBERT-base/large             | 256  | {(combined_df['tok_phobert']<=256).mean()*100:5.1f}% | {combined_df['tok_phobert'].max():,}")
print(f"Vietnamese_Embedding (AITeamVN)| 2048 | {(combined_df['tok_vn_embed']<=2048).mean()*100:5.1f}% | {combined_df['tok_vn_embed'].max():,}")
print(f"vietnamese-document-embedding  | 8192 | {(combined_df['tok_vn_doc']<=8192).mean()*100:5.1f}% | {combined_df['tok_vn_doc'].max():,}")
print("="*60)

"""# üìä B√°o C√°o Ph√¢n T√≠ch Token Count

## K·∫øt Qu·∫£ Th·ªëng K√™
```
============================================================
MODEL                          | MAX  | % FIT  | MAX IN DATA
============================================================
PhoBERT-base/large             | 256  |   1.2% | 2,713
Vietnamese_Embedding (AITeamVN)| 2048 | 100.0% | 1,999
vietnamese-document-embedding  | 8192 | 100.0% | 1,999
============================================================
```

---

## 1. Gi·∫£i Th√≠ch C√°c C·ªôt

| C·ªôt | √ù nghƒ©a |
|-----|---------|
| **MODEL** | T√™n model embedding ƒë∆∞·ª£c ƒë√°nh gi√° |
| **MAX** | S·ªë tokens t·ªëi ƒëa m√† model c√≥ th·ªÉ x·ª≠ l√Ω trong 1 l·∫ßn |
| **% FIT** | Ph·∫ßn trƒÉm vƒÉn b·∫£n trong dataset n·∫±m trong gi·ªõi h·∫°n MAX |
| **MAX IN DATA** | S·ªë tokens c·ªßa vƒÉn b·∫£n d√†i nh·∫•t trong dataset |

---

## 2. Ph√¢n T√≠ch T·ª´ng Model

### 2.1. PhoBERT-base/large

| Th√¥ng s·ªë | Gi√° tr·ªã |
|----------|---------|
| Gi·ªõi h·∫°n MAX | 256 tokens |
| % FIT | 1.2% |
| MAX IN DATA | 2,713 tokens |

**Gi·∫£i th√≠ch:**
- ‚úÖ **1.2%** vƒÉn b·∫£n c√≥ ƒë·ªô d√†i ‚â§ 256 tokens ‚Üí Gi·ªØ nguy√™n, kh√¥ng m·∫•t th√¥ng tin
- ‚ùå **98.8%** vƒÉn b·∫£n c√≥ ƒë·ªô d√†i > 256 tokens ‚Üí B·ªã **truncate (c·∫Øt b·ªè)** ph·∫ßn v∆∞·ª£t qu√°

**H·∫≠u qu·∫£ v·ªõi 98.8% vƒÉn b·∫£n b·ªã c·∫Øt:**
```
VƒÉn b·∫£n 2,713 tokens:  [1] [2] [3] ... [256] | [257] [258] ... [2,713]
                                        ‚Üë
                                   C·∫Øt t·∫°i ƒë√¢y
                         
Gi·ªØ l·∫°i: [1]...[256]     ‚Üí 9% n·ªôi dung
M·∫•t ƒëi:  [257]...[2,713] ‚Üí 91% n·ªôi dung ‚ùå
```

**‚ö†Ô∏è K·∫øt lu·∫≠n: KH√îNG N√äN D√ôNG PhoBERT cho dataset n√†y**

---

### 2.2. Vietnamese_Embedding (AITeamVN)

| Th√¥ng s·ªë | Gi√° tr·ªã |
|----------|---------|
| Gi·ªõi h·∫°n MAX | 2048 tokens |
| % FIT | 100% |
| MAX IN DATA | 1,999 tokens |

**Gi·∫£i th√≠ch:**
- ‚úÖ **100%** vƒÉn b·∫£n n·∫±m trong gi·ªõi h·∫°n
- ‚úÖ VƒÉn b·∫£n d√†i nh·∫•t (1,999) < Gi·ªõi h·∫°n (2,048)
- ‚úÖ **Kh√¥ng m·∫•t th√¥ng tin**
```
MAX IN DATA (1,999) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë MAX LIMIT (2,048)
                    ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 97.6% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí                   ‚Üê2.4%‚Üí
```

**‚úÖ K·∫øt lu·∫≠n: KHUY·∫æN NGH·ªä S·ª¨ D·ª§NG**

---

### 2.3. vietnamese-document-embedding

| Th√¥ng s·ªë | Gi√° tr·ªã |
|----------|---------|
| Gi·ªõi h·∫°n MAX | 8192 tokens |
| % FIT | 100% |
| MAX IN DATA | 1,999 tokens |

**Gi·∫£i th√≠ch:**
- ‚úÖ **100%** vƒÉn b·∫£n n·∫±m trong gi·ªõi h·∫°n
- ‚úÖ **Kh√¥ng m·∫•t th√¥ng tin**
- ‚ö†Ô∏è D∆∞ th·ª´a capacity (ch·ªâ s·ª≠ d·ª•ng 24% gi·ªõi h·∫°n)
```
MAX IN DATA (1,999) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë MAX LIMIT (8,192)
                    ‚Üê‚îÄ‚îÄ 24.4% ‚îÄ‚îÄ‚Üí                                       ‚Üê‚îÄ‚îÄ 75.6% d∆∞ ‚îÄ‚îÄ‚Üí
```

**‚úÖ K·∫øt lu·∫≠n: D√πng ƒë∆∞·ª£c, nh∆∞ng output dimension th·∫•p h∆°n (768 vs 1024)**

---

## 3. T·∫°i Sao MAX IN DATA Kh√°c Nhau (2,713 vs 1,999)?

M·ªói tokenizer t√°ch t·ª´ kh√°c nhau:

| C√¢u v√≠ d·ª• | PhoBERT | Vietnamese_Embedding |
|-----------|---------|---------------------|
| "TƒÉng 2.5%" | ["TƒÉng", "2", ".", "5", "%"] = 5 tokens | ["TƒÉng", "2.5%"] = 2 tokens |
| "VNM" | ["V", "N", "M"] = 3 tokens | ["VNM"] = 1 token |

‚Üí PhoBERT d√πng BPE, t√°ch nh·ªè h∆°n ‚Üí c√πng vƒÉn b·∫£n nh∆∞ng **nhi·ªÅu tokens h∆°n ~35%**

---

## 4. So S√°nh T·ªïng Quan

| Ti√™u ch√≠ | PhoBERT | Vietnamese_Embedding | vn-document-embedding |
|----------|:-------:|:--------------------:|:---------------------:|
| Max tokens | 256 | 2048 | 8192 |
| Output dim | 768/1024 | **1024** | 768 |
| % Data fit | 1.2% ‚ùå | **100%** ‚úÖ | **100%** ‚úÖ |
| M·∫•t th√¥ng tin? | **98.8%** ‚ùå | **Kh√¥ng** ‚úÖ | **Kh√¥ng** ‚úÖ |
| Ph√π h·ª£p? | ‚ùå Kh√¥ng | ‚úÖ **T·ªët nh·∫•t** | ‚úÖ T·ªët |

---

## 5. Minh H·ªça M·∫•t Th√¥ng Tin
```
VƒÉn b·∫£n g·ªëc: "Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n Vi·ªát Nam h√¥m nay... [n·ªôi dung d√†i] ...K·∫øt lu·∫≠n: KHUY·∫æN NGH·ªä MUA"
                            ‚Üì
              PhoBERT Tokenizer (t√°ch t·ª´)
                            ‚Üì
             T·ªïng: 964 tokens (v√≠ d·ª•)
                            ‚Üì
        PhoBERT ch·ªâ nh·∫≠n MAX 256 tokens
                            ‚Üì
Gi·ªØ l·∫°i: "Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n..."     ‚Üê 27% ƒë·∫ßu
M·∫•t ƒëi:  "...K·∫øt lu·∫≠n: KHUY·∫æN NGH·ªä MUA"  ‚Üê 73% cu·ªëi ‚ùå

‚Üí M·∫§T ph·∫ßn k·∫øt lu·∫≠n quan tr·ªçng nh·∫•t!
```

---

## 6. K·∫øt Lu·∫≠n & Khuy·∫øn Ngh·ªã

### ‚ùå KH√îNG d√πng PhoBERT
- 98.8% vƒÉn b·∫£n b·ªã c·∫Øt
- M·∫•t t·ªõi 91% n·ªôi dung ·ªü vƒÉn b·∫£n d√†i nh·∫•t
- Kh√¥ng ph√π h·ª£p v·ªõi dataset n√†y

### ‚úÖ KHUY·∫æN NGH·ªä: Vietnamese_Embedding (AITeamVN)

| L√Ω do |
|-------|
| 100% vƒÉn b·∫£n fit (kh√¥ng m·∫•t th√¥ng tin) |
| Output 1024 dimensions (ch·∫•t l∆∞·ª£ng cao) |
| Fine-tuned cho ti·∫øng Vi·ªát |
| VƒÉn b·∫£n d√†i nh·∫•t (1,999) v·ª´a ƒë·ªß gi·ªõi h·∫°n (2,048) |

---

## 7. L∆∞u √ù V·ªÅ Dataset

- **C·∫•u tr√∫c data:** 1 tin t·ª©c = 1 row (ƒë√∫ng c√°ch)
- **1 ng√†y c√≥ nhi·ªÅu tin:** L∆∞u th√†nh nhi·ªÅu rows ri√™ng bi·ªát
- **Kh√¥ng g·ªôp tin t·ª©c:** Gi·ªØ nguy√™n t·ª´ng b√†i ƒë·ªÉ embedding ch√≠nh x√°c
```
Ng√†y 2019-12-16:
‚îú‚îÄ‚îÄ Row 1: "BVSC nh·∫≠n Top 10 th∆∞∆°ng hi·ªáu..."  (400 tokens)
‚îú‚îÄ‚îÄ Row 2: "Th·ªã tr∆∞·ªùng ch·ª©ng kho√°n..."        (964 tokens)
‚îî‚îÄ‚îÄ M·ªói row ƒë∆∞·ª£c embedding ri√™ng bi·ªát
```

---

*B√°o c√°o ƒë∆∞·ª£c t·∫°o t·ª´ ph√¢n t√≠ch token count tr√™n dataset tin t·ª©c t√†i ch√≠nh*

#EDA dataset
"""

print("Ki·ªÉm tra th√¥ng tin DataFrame sau khi l·ªçc:")
display(combined_df.info())

# Rows thi·∫øu title
print("Rows thi·∫øu title:")
display(combined_df[combined_df['title'].isna()])

# Rows thi·∫øu content
print("\nRows thi·∫øu content:")
display(combined_df[combined_df['content'].isna()])

# ƒê·∫øm s·ªë tin t·ª©c m·ªói ng√†y
news_per_day = combined_df.groupby(combined_df.index.date).size()

print("TH·ªêNG K√ä S·ªê TIN T·ª®C M·ªñI NG√ÄY:")
print(news_per_day.describe())

print(f"\nMin: {news_per_day.min()} tin/ng√†y")
print(f"Max: {news_per_day.max()} tin/ng√†y")
print(f"Mean: {news_per_day.mean():.1f} tin/ng√†y")

# Ng√†y c√≥ nhi·ªÅu tin nh·∫•t
print(f"\nNg√†y c√≥ nhi·ªÅu tin nh·∫•t:")
print(news_per_day.nlargest(10))

# Ng√†y c√≥ √≠t tin nh·∫•t
print(f"\nNg√†y ch·ªâ c√≥ 1 tin:")
print(f"{(news_per_day == 1).sum()} ng√†y")

# Histogram
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.hist(news_per_day, bins=20, edgecolor='black')
plt.xlabel('S·ªë tin t·ª©c / ng√†y')
plt.ylabel('S·ªë ng√†y')
plt.title('Ph√¢n b·ªë s·ªë l∆∞·ª£ng tin t·ª©c m·ªói ng√†y')
plt.show()

# T·∫°o DataFrame th·ªëng k√™
summary = pd.DataFrame({
    'news_count': news_per_day
})

print(f"T·ªïng s·ªë ng√†y c√≥ tin: {len(news_per_day)}")
print(f"T·ªïng s·ªë tin t·ª©c: {news_per_day.sum()}")
print(f"\nPh√¢n b·ªë:")
print(f"  - Ng√†y c√≥ 1 tin:   {(news_per_day == 1).sum()}")
print(f"  - Ng√†y c√≥ 2-5 tin: {((news_per_day >= 2) & (news_per_day <= 5)).sum()}")
print(f"  - Ng√†y c√≥ >5 tin:  {(news_per_day > 5).sum()}")

num_dup_dates = combined_df.index[combined_df.index.duplicated()].nunique()
print("S·ªë date b·ªã tr√πng:", num_dup_dates)

dup_detail = (
    combined_df
    .index
    .value_counts()
    .loc[lambda x: x > 1]
)

display(dup_detail)

combined_df.head()